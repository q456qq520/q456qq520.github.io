<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://q456qq520.github.io</id>
    <title>LIKECAT</title>
    <updated>2023-07-20T05:16:47.755Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://q456qq520.github.io"/>
    <link rel="self" href="https://q456qq520.github.io/atom.xml"/>
    <subtitle>一条小咸鱼</subtitle>
    <logo>https://q456qq520.github.io/images/avatar.png</logo>
    <icon>https://q456qq520.github.io/favicon.ico</icon>
    <rights>All rights reserved 2023, LIKECAT</rights>
    <entry>
        <title type="html"><![CDATA[最佳实践-springboot参数校验效率提升]]></title>
        <id>https://q456qq520.github.io/post/zui-jia-shi-jian-springboot-can-shu-xiao-yan-xiao-lu-ti-sheng/</id>
        <link href="https://q456qq520.github.io/post/zui-jia-shi-jian-springboot-can-shu-xiao-yan-xiao-lu-ti-sheng/">
        </link>
        <updated>2023-07-20T03:03:56.000Z</updated>
        <content type="html"><![CDATA[<h2 id="背景">背景</h2>
<p>随着语法糖或者工具包集成越来越多，现在参数校验已经不需要像往常那样写一大串的ifelse判断逻辑了。</p>
<p>常用的就有springboot中validation的引入大大的减少了工作量。</p>
<p>一般就是在实体类中添加 @NotNull等注解，再在入口处添加@Validated 的方式实现参数校验。这其中其实还可以偷懒，就是利用aop，全局进行参数校验，再统一增强返回，就不需要一个一个的加@Validated 了。</p>
<h2 id="全局参数校验">全局参数校验</h2>
<pre><code class="language-java"> public static &lt;T&gt; void postMethodException(T t) {
        if (t == null) {
            return;
        }
        ValidatorFactory factory = Validation.buildDefaultValidatorFactory();
        Validator validator = factory.getValidator();
        Set&lt;ConstraintViolation&lt;T&gt;&gt; violations = validator.validate(t);
        String errorMsg = violations.stream().map(ConstraintViolation::getMessage).collect(Collectors.joining(&quot;&amp;&quot;));
        if (StringUtils.isNotBlank(errorMsg)) {
            throw new PrescriptionException(ResultEnum.INPUT_PARAM_ERROR.getCode(), errorMsg);
        }
    }

    /**
     * 全局异常捕获
     */
    @ExceptionHandler(value = Exception.class)
    @ResponseBody
    public ResponseResult&lt;?&gt; otherException(Exception e) {
        log.error(&quot;catch other exception, e:&quot;, e);
        return ResponseResultUtil.failure(&quot;系统异常&quot;, ResultEnum.FAILURE.getCode(), e.getMessage());
    }

}

</code></pre>
<p>这是aop中部分代码贴图，不知道大家有没有发现问题，其中<br>
ValidatorFactory factory = Validation.buildDefaultValidatorFactory();<br>
Validator validator = factory.getValidator();</p>
<p>会导致校验效率十分低下，因为会频繁的new 工厂对象，导致gc频繁，造成性能瓶颈。</p>
<h2 id="解决办法">解决办法</h2>
<p>办法很简单。</p>
<pre><code class="language-java">public class ValidationUtils {

    private static Validator validator = Validation
            .byProvider(HibernateValidator.class)
            .configure()
            .failFast(true)
            .buildValidatorFactory()
            .getValidator();


    public static &lt;T&gt; void validate(T obj) {
        validator.validate(obj);
    }

}
</code></pre>
<p>既然知道了原因，那就不要让它一直创建就好了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[最佳实践-数据库的模糊搜索]]></title>
        <id>https://q456qq520.github.io/post/zui-jia-shi-jian-shu-ju-ku-de-mo-hu-sou-suo/</id>
        <link href="https://q456qq520.github.io/post/zui-jia-shi-jian-shu-ju-ku-de-mo-hu-sou-suo/">
        </link>
        <updated>2023-07-20T01:11:04.000Z</updated>
        <content type="html"><![CDATA[<h2 id="一-常见场景">一 常见场景</h2>
<h3 id="11-数据库表创建">1.1 数据库表创建</h3>
<p>首先创建一个学生表并添加姓名的普通索引。</p>
<pre><code class="language-mysql">CREATE TABLE `school`.`student`  (
  `id` bigint(10) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `name` varchar(30) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL,
  `age` int(3) NULL DEFAULT NULL,
  `class_number` bigint(10) NULL DEFAULT NULL,
  `create_time` timestamp NULL DEFAULT CURRENT_TIMESTAMP,
  `update_time` timestamp NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `name`(`name`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;
</code></pre>
<p>并为其中填充几条数据：</p>
<pre><code class="language-mysql">INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (1, '张三', 10, 3, '2023-07-20 09:31:29', '2023-07-20 09:31:34');
INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (2, '李四', 11, 3, '2023-07-20 09:31:44', '2023-07-20 09:31:44');
INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (3, '王五', 10, 3, '2023-07-20 09:32:19', '2023-07-20 09:32:19');
INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (4, '盖伦', 9, 3, '2023-07-20 09:32:39', '2023-07-20 09:32:39');
INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (5, '普朗克', 10, 3, '2023-07-20 09:32:56', '2023-07-20 09:32:56');
INSERT INTO `school`.`student` (`id`, `name`, `age`, `class_number`, `create_time`, `update_time`) VALUES (6, '卡特', 12, 3, '2023-07-20 09:34:31', '2023-07-20 09:34:31');
</code></pre>
<h3 id="12-查询案例">1.2 查询案例</h3>
<p>假设现在有需要对<code>name</code>进行模糊查询，sql如下：</p>
<pre><code class="language-mysql">select * from student where name like '%王%';
</code></pre>
<h3 id="13-案例分析">1.3 案例分析</h3>
<p>用<code>EXPLAIN</code>看看执行计划：<br>
<img src="https://q456qq520.github.io/post-images/1689817827264.png" alt="" loading="lazy"></p>
<p>从图中发现是没有走索引的。那怎么办？索引岂不是白建了？</p>
<h3 id="14-解决方法">1.4 解决方法</h3>
<p>别慌，先给你支一招，如下图：<br>
<img src="https://q456qq520.github.io/post-images/1689817985469.png" alt="" loading="lazy"></p>
<p>走最左侧原则，去掉左边的通配符，就可以发现能走索引了。</p>
<p>产品：我要的就是要全模糊，不要一边模糊搜索。</p>
<p>那该怎么办呢？</p>
<h3 id="15-全文索引">1.5 全文索引</h3>
<p>这时候可以采用终极杀招，使用mysql 自带的全文索引。不懂全文索引的可以先去了解一下。简而言之能实现我们全模糊的要求还走索引，是一种空间换时间的典型。</p>
<pre><code class="language-mysql">create fulltext index name_fulltext on student(name);
</code></pre>
<p>首先建立一个name的全文索引。</p>
<p>接着采用全文索引提供的布尔查询(也可以在查询参数上加通配符*)：</p>
<pre><code class="language-mysql">select * from student where match(name) against('朗' IN BOOLEAN MODE);
select * from student where MATCH ( name ) against ( '王' IN NATURAL LANGUAGE MODE );
</code></pre>
<p>发现，结果啥也没有。什么情况？</p>
<p>这个问题有很多原因，其中最常见的就是<mark>最小搜索长度</mark>导致的。</p>
<p>MySQL 中的全文索引，有两个变量，最小搜索长度和最大搜索长度，对于长度小于最小搜索长度和大于最大搜索长度的词语，都不会被索引。通俗点就是说，想对一个词语使用全文索引搜索，那么这个词语的长度必须在以上两个变量的区间内。</p>
<p>这两个的默认值可以使用以下命令查看</p>
<pre><code class="language-mysql">show variables like '%ft%';
</code></pre>
<p>可以看到这两个变量在 MyISAM 和 InnoDB 两种存储引擎下的变量名和默认值</p>
<pre><code class="language-mysql">// MyISAM
ft_min_word_len = 4;
ft_max_word_len = 84;

// InnoDB
innodb_ft_min_token_size = 3;
innodb_ft_max_token_size = 84;
</code></pre>
<p>可以看到最小搜索长度 MyISAM 引擎下默认是 4，InnoDB 引擎下是 3，也即，MySQL 的全文索引只会对长度大于等于 4 或者 3 的词语建立索引，而刚刚搜索的长度大于等于 4。</p>
<h4 id="配置最小搜索长度">配置最小搜索长度</h4>
<p>全文索引的相关参数都无法进行动态修改，必须通过修改 MySQL 的配置文件来完成。修改最小搜索长度的值为 1，首先打开 MySQL 的配置文件 /etc/my.cnf，在 [mysqld] 的下面追加以下内容</p>
<pre><code class="language-mysql">[mysqld]
innodb_ft_min_token_size = 1
ft_min_word_len = 1
</code></pre>
<blockquote>
<p>可以使用以下命令查询配置文件路径<br>
mysql --help|grep 'my.cnf'</p>
</blockquote>
<p>然后重启 MySQL 服务器，并修复全文索引。<br>
⚠️注意，修改完参数以后，一定要修复下索引，不然参数不会生效。</p>
<p>两种方式，一种是重建索引，一种是执行命令。</p>
<pre><code class="language-mysql">repair table student quick;
</code></pre>
<p>MySQL 的全文索引最开始仅支持英语，因为英语的词与词之间有空格，使用空格作为分词的分隔符是很方便的。亚洲文字，比如汉语、日语、汉语等，是没有空格的，这就造成了一定的限制。不过 MySQL 5.7.6 开始，引入了一个 ngram 全文分析器来解决这个问题，并且对 MyISAM 和 InnoDB 引擎都有效。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1689820849998.png" alt="" loading="lazy"></figure>
<p>再次执行执行计划，可以看到现在是走索引了。</p>
<h3 id="16-elasticsearch">1.6 ElasticSearch</h3>
<p>还有一个执行效率更高更快的办法，那就是引入ES。这里不过多介绍，大概方案就是，用text类型对需要模糊查询对词进行分词，其次是利用match查询进行模糊查询。要达到模糊的效果，首先是要对中英文采取不同对分词器，然后利用match的<mark>Operator.AND</mark>熟悉进行查询。</p>
<blockquote>
<p>Operator.AND和Operator.OR是用于设置match查询的操作符的枚举类型。它们的区别在于如何处理多个查询词的匹配逻辑：<br>
Operator.AND：使用AND操作符，表示所有的查询词都必须出现在匹配的文档中。只有当文档中同时包含所有的查询词时，才会被匹配到。<br>
Operator.OR：使用OR操作符，表示只要文档中包含任何一个查询词，就会被匹配到。只要文档中包含至少一个查询词，就会被视为匹配。</p>
</blockquote>
<p>具体案例为：</p>
<pre><code class="language-json">{
  &quot;from&quot;: 0,
  &quot;size&quot;: 20,
  &quot;timeout&quot;: &quot;60s&quot;,
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: [
        {
          &quot;exists&quot;: {
            &quot;field&quot;: &quot;yb_code&quot;,
            &quot;boost&quot;: 1
          }
        },
        {
          &quot;match&quot;: {
            &quot;name&quot;: {
              &quot;query&quot;: &quot;高血压&quot;,
              &quot;operator&quot;: &quot;AND&quot;,
              &quot;prefix_length&quot;: 0,
              &quot;max_expansions&quot;: 50,
              &quot;fuzzy_transpositions&quot;: true,
              &quot;lenient&quot;: false,
              &quot;zero_terms_query&quot;: &quot;NONE&quot;,
              &quot;auto_generate_synonyms_phrase_query&quot;: true,
              &quot;boost&quot;: 1
            }
          }
        }
      ],
      &quot;filter&quot;: [
        {
          &quot;bool&quot;: {
            &quot;must&quot;: [
              {
                &quot;term&quot;: {
                  &quot;category&quot;: {
                    &quot;value&quot;: 1,
                    &quot;boost&quot;: 1
                  }
                }
              }
            ],
            &quot;adjust_pure_negative&quot;: true,
            &quot;boost&quot;: 1
          }
        }
      ],
      &quot;adjust_pure_negative&quot;: true,
      &quot;boost&quot;: 1
    }
  },
  &quot;sort&quot;: [
    {
      &quot;name_length&quot;: {
        &quot;order&quot;: &quot;asc&quot;
      }
    },
    {
      &quot;id&quot;: {
        &quot;order&quot;: &quot;asc&quot;
      }
    }
  ]
}
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1689821426314.png" alt="" loading="lazy"></figure>
<p>其中match可以用matchPhrase,但是效果没有match好，或者使用wildcardQuery这种通配符查询的方式也可以。</p>
<p>PS：<br>
如果想看某个字段分词情况，可以用下面的请求：<br>
GET  https://{ip}:{port}/index/_doc/{id}/_termvectors?fields={field}</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[秒杀系统常见问题]]></title>
        <id>https://q456qq520.github.io/post/miao-sha-xi-tong-chang-jian-wen-ti/</id>
        <link href="https://q456qq520.github.io/post/miao-sha-xi-tong-chang-jian-wen-ti/">
        </link>
        <updated>2023-05-19T02:02:57.000Z</updated>
        <content type="html"><![CDATA[<h2 id="超卖问题">超卖问题</h2>
<p>秒杀系统主要应用在商品抢购的场景，比如：</p>
<ul>
<li>电商抢购限量商品</li>
<li>卖演唱会的门票</li>
<li>火车票抢座<br>
…</li>
</ul>
<p>秒杀系统抽象来说就是以下几个步骤：</p>
<ul>
<li>用户选定商品下单</li>
<li>校验库存</li>
<li>扣库存</li>
<li>创建用户订单</li>
</ul>
<p>对于秒杀系统来说，严格防止超卖是一件十分重要的事情。下面我们看一下超卖问题出现的原因与解决办法。</p>
<h3 id="建立简易的数据库表结构">建立“简易”的数据库表结构</h3>
<p>一张库存表stock，一张订单表stock_order</p>
<pre><code class="language-java">-- ----------------------------
-- Table structure for stock
-- ----------------------------
DROP TABLE IF EXISTS `stock`;
CREATE TABLE `stock` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(50) NOT NULL DEFAULT '' COMMENT '名称',
  `count` int(11) NOT NULL COMMENT '库存',
  `sale` int(11) NOT NULL COMMENT '已售',
  `version` int(11) NOT NULL COMMENT '乐观锁，版本号',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Table structure for stock_order
-- ----------------------------
DROP TABLE IF EXISTS `stock_order`;
CREATE TABLE `stock_order` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `sid` int(11) NOT NULL COMMENT '库存ID',
  `name` varchar(30) NOT NULL DEFAULT '' COMMENT '商品名称',
  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '创建时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
</code></pre>
<h3 id="相应代码">相应代码</h3>
<h5 id="controller层代码">Controller层代码</h5>
<p>提供一个HTTP接口: 参数为商品的Id</p>
<pre><code class="language-java">@RequestMapping(&quot;/createWrongOrder/{sid}&quot;)
@ResponseBody
public String createWrongOrder(@PathVariable int sid) {
    LOGGER.info(&quot;购买物品编号sid=[{}]&quot;, sid);
    int id = 0;
    try {
        id = orderService.createWrongOrder(sid);
        LOGGER.info(&quot;创建订单id: [{}]&quot;, id);
    } catch (Exception e) {
        LOGGER.error(&quot;Exception&quot;, e);
    }
    return String.valueOf(id);
}
</code></pre>
<h5 id="service层代码">Service层代码</h5>
<pre><code class="language-java">@Override
public int createWrongOrder(int sid) throws Exception {
    //校验库存
    Stock stock = checkStock(sid);
    //扣库存
    saleStock(stock);
    //创建订单
    int id = createOrder(stock);
    return id;
}

private Stock checkStock(int sid) {
    Stock stock = stockService.getStockById(sid);
    if (stock.getSale().equals(stock.getCount())) {
        throw new RuntimeException(&quot;库存不足&quot;);
    }
    return stock;
}

private int saleStock(Stock stock) {
    stock.setSale(stock.getSale() + 1);
    return stockService.updateStockById(stock);
}

private int createOrder(Stock stock) {
    StockOrder order = new StockOrder();
    order.setSid(stock.getId());
    order.setName(stock.getName());
    int id = orderMapper.insertSelective(order);
    return id;
}
</code></pre>
<h4 id="发起并发购买请求">发起并发购买请求</h4>
<blockquote>
<p>https://www.cnblogs.com/stulzq/p/8971531.html<br>
在JMeter里启动1000个线程，无延迟同时访问接口。模拟1000个人，抢购100个产品的场景。点击启动。</p>
</blockquote>
<p>结果卖出了14个，库存减少了14个，但是每个请求Spring都处理了，创建了1000个订单。</p>
<h4 id="避免超卖问题更新商品库存的版本号">避免超卖问题：更新商品库存的版本号</h4>
<p>为了解决上面的超卖问题，我们当然可以在Service层给更新表添加一个事务，这样每个线程更新请求的时候都会先去锁表的这一行（悲观锁），更新完库存后再释放锁。可这样就太慢了，1000个线程可等不及。</p>
<p>我们需要乐观锁。</p>
<p>一个最简单的办法就是，给每个商品库存一个版本号version字段</p>
<p>我们修改代码：</p>
<h5 id="controller层">Controller层</h5>
<pre><code class="language-java">/**
 * 乐观锁更新库存
 * @param sid
 * @return
 */
@RequestMapping(&quot;/createOptimisticOrder/{sid}&quot;)
@ResponseBody
public String createOptimisticOrder(@PathVariable int sid) {
    int id;
    try {
        id = orderService.createOptimisticOrder(sid);
        LOGGER.info(&quot;购买成功，剩余库存为: [{}]&quot;, id);
    } catch (Exception e) {
        LOGGER.error(&quot;购买失败：[{}]&quot;, e.getMessage());
        return &quot;购买失败，库存不足&quot;;
    }
    return String.format(&quot;购买成功，剩余库存为：%d&quot;, id);
</code></pre>
<h5 id="service层">Service层</h5>
<pre><code class="language-java">@Override
public int createOptimisticOrder(int sid) throws Exception {
    //校验库存
    Stock stock = checkStock(sid);
    //乐观锁更新库存
    saleStockOptimistic(stock);
    //创建订单
    int id = createOrder(stock);
    return stock.getCount() - (stock.getSale()+1);
}

private void saleStockOptimistic(Stock stock) {
    LOGGER.info(&quot;查询数据库，尝试更新库存&quot;);
    int count = stockService.updateStockByOptimistic(stock);
    if (count == 0){
        throw new RuntimeException(&quot;并发更新库存失败，version不匹配&quot;) ;
    }
}
</code></pre>
<h5 id="mapper">Mapper</h5>
<pre><code class="language-xml">&lt;update id=&quot;updateByOptimistic&quot; parameterType=&quot;cn.monitor4all.miaoshadao.dao.Stock&quot;&gt;
    update stock
    &lt;set&gt;
      sale = sale + 1,
      version = version + 1,
    &lt;/set&gt;
    WHERE id = #{id,jdbcType=INTEGER}
    AND version = #{version,jdbcType=INTEGER}
  &lt;/update&gt;
</code></pre>
<p>我们在实际减库存的SQL操作中，首先判断version是否是我们查询库存时候的version，如果是，扣减库存，成功抢购。如果发现version变了，则不更新数据库，返回抢购失败。</p>
<p>再次打开JMeter，把库存恢复为100，清空订单表，发起1000次请求。</p>
<p>这次的结果是：</p>
<p>卖出去了39个，version更新为了39,同时创建了39个订单。我们没有超卖，可喜可贺。</p>
<p>由于并发访问的原因，很多线程更新库存失败了，所以在我们这种设计下，1000个人真要是同时发起购买，只有39个幸运儿能够买到东西，但是我们防止了超卖。</p>
<h2 id="令牌桶限流-再谈超卖">令牌桶限流 + 再谈超卖</h2>
<h3 id="接口限流">接口限流</h3>
<p>在面临高并发的请购请求时，我们如果不对接口进行限流，可能会对后台系统造成极大的压力。尤其是对于下单的接口，过多的请求打到数据库会对系统的稳定性造成影响。</p>
<p>所以秒杀系统会尽量选择独立于公司其他后端系统之外进行单独部署，以免秒杀业务崩溃影响到其他系统。</p>
<p>除了独立部署秒杀业务之外，我们能够做的就是尽量让后台系统稳定优雅的处理大量请求。</p>
<h5 id="令牌桶算法与漏桶算法">令牌桶算法与漏桶算法</h5>
<blockquote>
<p>漏桶算法思路很简单，水（请求）先进入到漏桶里，漏桶以一定的速度出水，当水流入速度过大会直接溢出，可以看出漏桶算法能强行限制数据的传输速率。</p>
</blockquote>
<p>令牌桶算法不能与另外一种常见算法漏桶算法相混淆。这两种算法的主要区别在于：</p>
<p>漏桶算法能够强行限制数据的传输速率，而令牌桶算法在能够限制数据的平均传输速率外，还允许某种程度的突发传输。在令牌桶算法中，只要令牌桶中存在令牌，那么就允许突发地传输数据直到达到用户配置的门限，因此它适合于具有突发特性的流量。</p>
<h5 id="使用guava的ratelimiter实现令牌桶限流接口">使用Guava的RateLimiter实现令牌桶限流接口</h5>
<p>Guava是只提供了令牌桶的一种实现，实际项目中肯定还要根据需求来使用或者自己实现，大家可以看看这篇文章：<br>
https://segmentfault.com/a/1190000012875897</p>
<blockquote>
<p>OrderController</p>
</blockquote>
<pre><code class="language-java">@Controller
public class OrderController {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderController.class);

    @Autowired
    private StockService stockService;

    @Autowired
    private OrderService orderService;

    //每秒放行10个请求
    RateLimiter rateLimiter = RateLimiter.create(10);

    @RequestMapping(&quot;/createWrongOrder/{sid}&quot;)
    @ResponseBody
    public String createWrongOrder(@PathVariable int sid) {
        int id = 0;
        try {
            id = orderService.createWrongOrder(sid);
            LOGGER.info(&quot;创建订单id: [{}]&quot;, id);
        } catch (Exception e) {
            LOGGER.error(&quot;Exception&quot;, e);
        }
        return String.valueOf(id);
    }

    /**
     * 乐观锁更新库存 + 令牌桶限流
     * @param sid
     * @return
     */
    @RequestMapping(&quot;/createOptimisticOrder/{sid}&quot;)
    @ResponseBody
    public String createOptimisticOrder(@PathVariable int sid) {
        // 阻塞式获取令牌
        //LOGGER.info(&quot;等待时间&quot; + rateLimiter.acquire());
        // 非阻塞式获取令牌
        if (!rateLimiter.tryAcquire(1000, TimeUnit.MILLISECONDS)) {
            LOGGER.warn(&quot;你被限流了，真不幸，直接返回失败&quot;);
            return &quot;购买失败，库存不足&quot;;
        }
        int id;
        try {
            id = orderService.createOptimisticOrder(sid);
            LOGGER.info(&quot;购买成功，剩余库存为: [{}]&quot;, id);
        } catch (Exception e) {
            LOGGER.error(&quot;购买失败：[{}]&quot;, e.getMessage());
            return &quot;购买失败，库存不足&quot;;
        }
        return String.format(&quot;购买成功，剩余库存为：%d&quot;, id);
    }
}
</code></pre>
<p>代码中，RateLimiter rateLimiter = RateLimiter.create(10);这里初始化了令牌桶类，每秒放行10个请求。</p>
<p>在接口中，可以看到有两种使用方法：</p>
<ol>
<li>阻塞式获取令牌：请求进来后，若令牌桶里没有足够的令牌，就在这里阻塞住，等待令牌的发放。</li>
<li>非阻塞式获取令牌：请求进来后，若令牌桶里没有足够的令牌，会尝试等待设置好的时间（这里写了1000ms），其会自动判断在1000ms后，这个请求能不能拿到令牌，如果不能拿到，直接返回抢购失败。如果timeout设置为0，则等于阻塞时获取令牌。</li>
</ol>
<h3 id="再谈防止超卖">再谈防止超卖</h3>
<p>讲完了令牌桶限流算法，我们再回头思考超卖的问题，在海量请求的场景下，如果像第一篇文章那样的使用乐观锁，会导致大量的请求返回抢购失败，用户体验极差。</p>
<p>然而使用悲观锁，比如数据库事务，则可以让数据库一个个处理库存数修改，修改成功后再迎接下一个请求，所以在不同情况下，应该根据实际情况使用悲观锁和乐观锁。</p>
<h4 id="实现不需要版本号字段的乐观锁">实现不需要版本号字段的乐观锁</h4>
<pre><code class="language-xml">&lt;update id=&quot;updateByOptimistic&quot; parameterType=&quot;cn.monitor4all.miaoshadao.dao.Stock&quot;&gt;
    update stock
    &lt;set&gt;
      sale = sale + 1,
    &lt;/set&gt;
    WHERE id = #{id,jdbcType=INTEGER}
    AND sale = #{sale,jdbcType=INTEGER}
&lt;/update&gt;
</code></pre>
<h4 id="实现悲观锁">实现悲观锁</h4>
<blockquote>
<p>Controller</p>
</blockquote>
<pre><code class="language-java">/**
 * 事务for update更新库存
 * @param sid
 * @return
 */
@RequestMapping(&quot;/createPessimisticOrder/{sid}&quot;)
@ResponseBody
public String createPessimisticOrder(@PathVariable int sid) {
    int id;
    try {
        id = orderService.createPessimisticOrder(sid);
        LOGGER.info(&quot;购买成功，剩余库存为: [{}]&quot;, id);
    } catch (Exception e) {
        LOGGER.error(&quot;购买失败：[{}]&quot;, e.getMessage());
        return &quot;购买失败，库存不足&quot;;
    }
    return String.format(&quot;购买成功，剩余库存为：%d&quot;, id);
}
</code></pre>
<p>在Service中,给该卖商品流程加上事务:</p>
<pre><code class="language-java">@Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED)
@Override
public int createPessimisticOrder(int sid){
    //校验库存(悲观锁for update)
    Stock stock = checkStockForUpdate(sid);
    //更新库存
    saleStock(stock);
    //创建订单
    int id = createOrder(stock);
    return stock.getCount() - (stock.getSale());
}

/**
 * 检查库存 ForUpdate
 * @param sid
 * @return
 */
private Stock checkStockForUpdate(int sid) {
    Stock stock = stockService.getStockByIdForUpdate(sid);
    if (stock.getSale().equals(stock.getCount())) {
        throw new RuntimeException(&quot;库存不足&quot;);
    }
    return stock;
}

/**
 * 更新库存
 * @param stock
 */
private void saleStock(Stock stock) {
    stock.setSale(stock.getSale() + 1);
    stockService.updateStockById(stock);
}

/**
 * 创建订单
 * @param stock
 * @return
 */
private int createOrder(Stock stock) {
    StockOrder order = new StockOrder();
    order.setSid(stock.getId());
    order.setName(stock.getName());
    int id = orderMapper.insertSelective(order);
    return id;
}
</code></pre>
<p>这里使用Spring的事务，@Transactional(rollbackFor = Exception.class, propagation = Propagation.REQUIRED)，如果遇到回滚，则返回Exception，并且事务传播使用PROPAGATION_REQUIRED–支持当前事务，如果当前没有事务，就新建一个事务。</p>
<p>所以，悲观锁在大量请求的请求下，有着更好的卖出成功率。但是需要注意的是，如果请求量巨大，悲观锁会导致后面的请求进行了长时间的阻塞等待，用户就必须在页面等待，很像是“假死”，可以通过配合令牌桶限流，或者是给用户显著的等待提示来优化。</p>
<h2 id="抢购接口隐藏-单用户限制频率">抢购接口隐藏 + 单用户限制频率</h2>
<h3 id="抢购接口隐藏">抢购接口隐藏</h3>
<p>抢购接口隐藏（接口加盐）的具体做法：</p>
<ul>
<li>每次点击秒杀按钮，先从服务器获取一个秒杀验证值（接口内判断是否到秒杀时间）。</li>
<li>Redis以缓存用户ID和商品ID为Key，秒杀地址为Value缓存验证值</li>
<li>用户请求秒杀商品的时候，要带上秒杀验证值进行校验。</li>
</ul>
<p>理论上来说在访问接口的时间上受到了限制，并且我们还能通过在验证值接口增加更复杂的逻辑，让获取验证值的接口并不快速返回验证值，进一步拉平普通用户和坏蛋们的下单时刻。所以接口加盐还是有用的！</p>
<blockquote>
<p>加盐代码逻辑实现<br>
代码还是使用之前的项目，我们在其上面增加两个接口：</p>
</blockquote>
<ul>
<li>获取验证值接口</li>
<li>携带验证值下单接口</li>
</ul>
<p>由于之前我们只有两个表，一个stock表放库存商品，一个stockOrder订单表，放订购成功的记录。但是这次涉及到了用户，所以我们新增用户表，并且添加一个用户张三。并且在订单表中，不仅要记录商品id，同时要写入用户id。</p>
<p>整个SQL结构如下，讲究一个简洁，暂时不加入别的多余字段：</p>
<pre><code class="language-sql">-- ----------------------------
-- Table structure for stock
-- ----------------------------
DROP TABLE IF EXISTS `stock`;
CREATE TABLE `stock` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(50) NOT NULL DEFAULT '' COMMENT '名称',
  `count` int(11) NOT NULL COMMENT '库存',
  `sale` int(11) NOT NULL COMMENT '已售',
  `version` int(11) NOT NULL COMMENT '乐观锁，版本号',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of stock
-- ----------------------------
INSERT INTO `stock` VALUES ('1', 'iphone', '50', '0', '0');
INSERT INTO `stock` VALUES ('2', 'mac', '10', '0', '0');

-- ----------------------------
-- Table structure for stock_order
-- ----------------------------
DROP TABLE IF EXISTS `stock_order`;
CREATE TABLE `stock_order` (
  `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `sid` int(11) NOT NULL COMMENT '库存ID',
  `name` varchar(30) NOT NULL DEFAULT '' COMMENT '商品名称',
  `user_id` int(11) NOT NULL DEFAULT '0',
  `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '创建时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of stock_order
-- ----------------------------

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT,
  `user_name` varchar(255) NOT NULL DEFAULT '',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES ('1', '张三');
</code></pre>
<h5 id="获取验证值接口">获取验证值接口</h5>
<p>该接口要求传用户id和商品id，返回验证值，并且该验证值</p>
<p>Controller中添加方法：</p>
<pre><code class="language-java">/**
 * 获取验证值
 * @return
 */
@RequestMapping(value = &quot;/getVerifyHash&quot;, method = {RequestMethod.GET})
@ResponseBody
public String getVerifyHash(@RequestParam(value = &quot;sid&quot;) Integer sid,
                            @RequestParam(value = &quot;userId&quot;) Integer userId) {
    String hash;
    try {
        hash = userService.getVerifyHash(sid, userId);
    } catch (Exception e) {
        LOGGER.error(&quot;获取验证hash失败，原因：[{}]&quot;, e.getMessage());
        return &quot;获取验证hash失败&quot;;
    }
    return String.format(&quot;请求抢购验证hash值为：%s&quot;, hash);
}
</code></pre>
<p>UserService中添加方法：</p>
<pre><code class="language-java">@Override
public String getVerifyHash(Integer sid, Integer userId) throws Exception {

    // 验证是否在抢购时间内
    LOGGER.info(&quot;请自行验证是否在抢购时间内&quot;);


    // 检查用户合法性
    User user = userMapper.selectByPrimaryKey(userId.longValue());
    if (user == null) {
        throw new Exception(&quot;用户不存在&quot;);
    }
    LOGGER.info(&quot;用户信息：[{}]&quot;, user.toString());

    // 检查商品合法性
    Stock stock = stockService.getStockById(sid);
    if (stock == null) {
        throw new Exception(&quot;商品不存在&quot;);
    }
    LOGGER.info(&quot;商品信息：[{}]&quot;, stock.toString());

    // 生成hash
    String verify = SALT + sid + userId;
    String verifyHash = DigestUtils.md5DigestAsHex(verify.getBytes());

    // 将hash和用户商品信息存入redis
    String hashKey = CacheKey.HASH_KEY.getKey() + &quot;_&quot; + sid + &quot;_&quot; + userId;
    stringRedisTemplate.opsForValue().set(hashKey, verifyHash, 3600, TimeUnit.SECONDS);
    LOGGER.info(&quot;Redis写入：[{}] [{}]&quot;, hashKey, verifyHash);
    return verifyHash;
}
</code></pre>
<p>一个Cache常量枚举类CacheKey：</p>
<pre><code class="language-java">package cn.monitor4all.miaoshadao.utils;

public enum CacheKey {
    HASH_KEY(&quot;miaosha_hash&quot;),
    LIMIT_KEY(&quot;miaosha_limit&quot;);

    private String key;

    private CacheKey(String key) {
        this.key = key;
    }
    public String getKey() {
        return key;
    }
}
</code></pre>
<p>可以看到在Service中，我们拿到用户id和商品id后，会检查商品和用户信息是否在表中存在，并且会验证现在的时间（我这里为了简化，只是写了一行LOGGER，大家可以根据需求自行实现）。在这样的条件过滤下，才会给出hash值。并且将Hash值写入了Redis中，缓存3600秒（1小时），如果用户拿到这个hash值一小时内没下单，则需要重新获取hash值。</p>
<p>想一下，这个hash值，如果每次都按照商品+用户的信息来md5，是不是不太安全呢。毕竟用户id并不一定是用户不知道的（就比如我这种用自增id存储的，肯定不安全），而商品id，万一也泄露了出去，那么如果再知到我们是简单的md5，那直接就把hash算出来了！</p>
<p>在代码里，我给hash值加了个前缀，也就是一个salt（盐），相当于给这个固定的字符串撒了一把盐，这个盐是HASH_KEY(&quot;miaosha_hash&quot;)，写死在了代码里。这样黑产只要不猜到这个盐，就没办法算出来hash值。</p>
<p>这也只是一种例子，实际中，你可以把盐放在其他地方， 并且不断变化，或者结合时间戳，这样就算自己的程序员也没法知道hash值的原本字符串是什么了。</p>
<h5 id="携带验证值下单接口">携带验证值下单接口</h5>
<p>用户在前台拿到了验证值后，点击下单按钮，前端携带着特征值，即可进行下单操作。</p>
<p>Controller中添加方法：</p>
<pre><code class="language-java">/**
 * 要求验证的抢购接口
 * @param sid
 * @return
 */
@RequestMapping(value = &quot;/createOrderWithVerifiedUrl&quot;, method = {RequestMethod.GET})
@ResponseBody
public String createOrderWithVerifiedUrl(@RequestParam(value = &quot;sid&quot;) Integer sid,
                                         @RequestParam(value = &quot;userId&quot;) Integer userId,
                                         @RequestParam(value = &quot;verifyHash&quot;) String verifyHash) {
    int stockLeft;
    try {
        stockLeft = orderService.createVerifiedOrder(sid, userId, verifyHash);
        LOGGER.info(&quot;购买成功，剩余库存为: [{}]&quot;, stockLeft);
    } catch (Exception e) {
        LOGGER.error(&quot;购买失败：[{}]&quot;, e.getMessage());
        return e.getMessage();
    }
    return String.format(&quot;购买成功，剩余库存为：%d&quot;, stockLeft);
}
</code></pre>
<p>OrderService中添加方法：</p>
<pre><code class="language-java">@Override
public int createVerifiedOrder(Integer sid, Integer userId, String verifyHash) throws Exception {

    // 验证是否在抢购时间内
    LOGGER.info(&quot;请自行验证是否在抢购时间内,假设此处验证成功&quot;);

    // 验证hash值合法性
    String hashKey = CacheKey.HASH_KEY.getKey() + &quot;_&quot; + sid + &quot;_&quot; + userId;
    String verifyHashInRedis = stringRedisTemplate.opsForValue().get(hashKey);
    if (!verifyHash.equals(verifyHashInRedis)) {
        throw new Exception(&quot;hash值与Redis中不符合&quot;);
    }
    LOGGER.info(&quot;验证hash值合法性成功&quot;);

    // 检查用户合法性
    User user = userMapper.selectByPrimaryKey(userId.longValue());
    if (user == null) {
        throw new Exception(&quot;用户不存在&quot;);
    }
    LOGGER.info(&quot;用户信息验证成功：[{}]&quot;, user.toString());

    // 检查商品合法性
    Stock stock = stockService.getStockById(sid);
    if (stock == null) {
        throw new Exception(&quot;商品不存在&quot;);
    }
    LOGGER.info(&quot;商品信息验证成功：[{}]&quot;, stock.toString());

    //乐观锁更新库存
    saleStockOptimistic(stock);
    LOGGER.info(&quot;乐观锁更新库存成功&quot;);

    //创建订单
    createOrderWithUserInfo(stock, userId);
    LOGGER.info(&quot;创建订单成功&quot;);

    return stock.getCount() - (stock.getSale()+1);
}
</code></pre>
<h3 id="单用户限制频率">单用户限制频率</h3>
<p>假设我们做好了接口隐藏，但是像我上面说的，总有无聊的人会写一个复杂的脚本，先请求hash值，再立刻请求购买，如果你的app下单按钮做的很差，大家都要开抢后0.5秒才能请求成功，那可能会让脚本依然能够在大家前面抢购成功。</p>
<p>我们需要在做一个额外的措施，来限制单个用户的抢购频率。</p>
<p>其实很简单的就能想到用redis给每个用户做访问统计，甚至是带上商品id，对单个商品做访问统计，这都是可行的。</p>
<p>我们先实现一个对用户的访问频率限制，我们在用户申请下单时，检查用户的访问次数，超过访问次数，则不让他下单！</p>
<h5 id="使用redismemcached">使用Redis/Memcached</h5>
<p>我们使用外部缓存来解决问题，这样即便是分布式的秒杀系统，请求被随意分流的情况下，也能做到精准的控制每个用户的访问次数。</p>
<p>Controller中添加方法：</p>
<pre><code class="language-java">/**
 * 要求验证的抢购接口 + 单用户限制访问频率
 * @param sid
 * @return
 */
@RequestMapping(value = &quot;/createOrderWithVerifiedUrlAndLimit&quot;, method = {RequestMethod.GET})
@ResponseBody
public String createOrderWithVerifiedUrlAndLimit(@RequestParam(value = &quot;sid&quot;) Integer sid,
                                                 @RequestParam(value = &quot;userId&quot;) Integer userId,
                                                 @RequestParam(value = &quot;verifyHash&quot;) String verifyHash) {
    int stockLeft;
    try {
        int count = userService.addUserCount(userId);
        LOGGER.info(&quot;用户截至该次的访问次数为: [{}]&quot;, count);
        boolean isBanned = userService.getUserIsBanned(userId);
        if (isBanned) {
            return &quot;购买失败，超过频率限制&quot;;
        }
        stockLeft = orderService.createVerifiedOrder(sid, userId, verifyHash);
        LOGGER.info(&quot;购买成功，剩余库存为: [{}]&quot;, stockLeft);
    } catch (Exception e) {
        LOGGER.error(&quot;购买失败：[{}]&quot;, e.getMessage());
        return e.getMessage();
    }
    return String.format(&quot;购买成功，剩余库存为：%d&quot;, stockLeft);
}
</code></pre>
<p>UserService中增加两个方法：</p>
<ul>
<li>addUserCount：每当访问订单接口，则增加一次访问次数，写入Redis</li>
<li>getUserIsBanned：从Redis读出该用户的访问次数，超过10次则不让购买了！不能让张三做法外狂徒。</li>
</ul>
<pre><code class="language-java">@Override
    public int addUserCount(Integer userId) throws Exception {
        String limitKey = CacheKey.LIMIT_KEY.getKey() + &quot;_&quot; + userId;
        String limitNum = stringRedisTemplate.opsForValue().get(limitKey);
        int limit = -1;
        if (limitNum == null) {
            stringRedisTemplate.opsForValue().set(limitKey, &quot;0&quot;, 3600, TimeUnit.SECONDS);
        } else {
            limit = Integer.parseInt(limitNum) + 1;
            stringRedisTemplate.opsForValue().set(limitKey, String.valueOf(limit), 3600, TimeUnit.SECONDS);
        }
        return limit;
    }

    @Override
    public boolean getUserIsBanned(Integer userId) {
        String limitKey = CacheKey.LIMIT_KEY.getKey() + &quot;_&quot; + userId;
        String limitNum = stringRedisTemplate.opsForValue().get(limitKey);
        if (limitNum == null) {
            LOGGER.error(&quot;该用户没有访问申请验证值记录，疑似异常&quot;);
            return true;
        }
        return Integer.parseInt(limitNum) &gt; ALLOW_COUNT;
    }
</code></pre>
<h5 id="能否不用redismemcached实现用户访问频率统计">能否不用Redis/Memcached实现用户访问频率统计</h5>
<p>如果你说你不愿意用redis，有什么办法能够实现访问频率统计吗，有呀，如果你放弃分布式的部署服务，那么你可以在内存中存储访问次数，比如：</p>
<ul>
<li>Google Guava的内存缓存</li>
<li>状态模式</li>
</ul>
<h2 id="缓存与数据库双写问题的争议">缓存与数据库双写问题的争议</h2>
<h3 id="缓存热点数据">缓存热点数据</h3>
<p>在秒杀实际的业务中，一定有很多需要做缓存的场景，比如售卖的商品，包括名称，详情等。访问量很大的数据，可以算是“热点”数据了，尤其是一些读取量远大于写入量的数据，更应该被缓存，而不应该让请求打到数据库上。</p>
<p>缓存量大但又不常变化的数据，比如详情，评论等。对于那些经常变化的数据，其实并不适合缓存，一方面会增加系统的复杂性（缓存的更新，缓存脏数据），另一方面也给系统带来一定的不稳定性（缓存系统的维护）。</p>
<p>「但一些极端情况下，你需要将一些会变动的数据进行缓存，比如想要页面显示准实时的库存数，或者其他一些特殊业务场景。这时候你需要保证缓存不能（一直）有脏数据。</p>
<p>上缓存的优点：</p>
<ul>
<li>能够缩短服务的响应时间，给用户带来更好的体验。</li>
<li>能够增大系统的吞吐量，依然能够提升用户体验。</li>
<li>减轻数据库的压力，防止高峰期数据库被压垮，导致整个线上服务BOOM！</li>
</ul>
<p>上了缓存，也会引入很多额外的问题：</p>
<ul>
<li>缓存有多种选型，是内存缓存，memcached还是redis，你是否都熟悉，如果不熟悉，无疑增加了维护的难度（本来是个纯洁的数据库系统）。</li>
<li>缓存系统也要考虑分布式，比如redis的分布式缓存还会有很多坑，无疑增加了系统的复杂性。</li>
<li>在特殊场景下，如果对缓存的准确性有非常高的要求，就必须考虑「缓存和数据库的一致性问题」。</li>
</ul>
<h3 id="缓存和数据库双写一致性">缓存和数据库双写一致性</h3>
<h5 id="不使用更新缓存而是删除缓存">不使用更新缓存而是删除缓存</h5>
<p>原因一：线程安全角度<br>
同时有请求A和请求B进行更新操作，那么会出现</p>
<p>（1）线程A更新了数据库<br>
（2）线程B更新了数据库<br>
（3）线程B更新了缓存<br>
（4）线程A更新了缓存</p>
<p>这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。</p>
<p>原因二：业务场景角度<br>
有如下两点：</p>
<p>（1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。<br>
（2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。</p>
<blockquote>
<p>其实如果业务非常简单，只是去数据库拿一个值，写入缓存，那么更新缓存也是可以的。但是，淘汰缓存操作简单，并且带来的副作用只是增加了一次cache miss，建议作为通用的处理方式。</p>
</blockquote>
<h5 id="先删除缓存还是先操作数据库">先删除缓存，还是先操作数据库?</h5>
<p>对于一个不能保证事务性的操作，一定涉及“哪个任务先做，哪个任务后做”的问题，解决这个问题的方向是：如果出现不一致，谁先做对业务的影响较小，就谁先执行。</p>
<p>假设先淘汰缓存，再写数据库：第一步淘汰缓存成功，第二步写数据库失败，则只会引发一次Cache miss。</p>
<p>假设先写数据库，再淘汰缓存：第一步写数据库操作成功，第二步淘汰缓存失败，则会出现DB中是新数据，Cache中是旧数据，数据不一致。</p>
<p>假如先删缓存，再更新数据库，该方案会导致请求数据不一致，比如同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:</p>
<p>（1）请求A进行写操作，删除缓存<br>
（2）请求B查询发现缓存不存在<br>
（3）请求B去数据库查询得到旧值<br>
（4）请求B将旧值写入缓存<br>
（5）请求A将新值写入数据库</p>
<p>上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。</p>
<p><mark>所以先删缓存，再更新数据库并不是一劳永逸的解决方案，再看看先更新数据库，再删缓存</mark></p>
<p>先更新数据库，再删缓存这种情况不存在并发问题么？不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生</p>
<p>（1）缓存刚好失效<br>
（2）请求A查询数据库，得一个旧值<br>
（3）请求B将新值写入数据库<br>
（4）请求B删除缓存<br>
（5）请求A将查到的旧值写入缓存</p>
<p>如果发生上述情况，确实是会发生脏数据。然而，发生这种情况的概率又有多少呢？发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。</p>
<p>所以，如果你想实现基础的缓存数据库双写一致的逻辑，那么在大多数情况下，在不想做过多设计，增加太大工作量的情况下，请<mark>先更新数据库，再删缓存</mark>!</p>
<h3 id="我一定要数据库和缓存数据一致怎么办">我一定要数据库和缓存数据一致怎么办</h3>
<p><mark>没有办法做到绝对的一致性，这是由CAP理论决定的，缓存系统适用的场景就是非强一致性的场景，所以它属于CAP中的AP</mark></p>
<p>所以，我们得委曲求全，可以去做到BASE理论中说的<code>「最终一致性」</code>。</p>
<blockquote>
<p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性</p>
</blockquote>
<h5 id="延时双删">延时双删</h5>
<p>上文我们提到，在先删除缓存，再更新数据库的情况下，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。那么延时双删怎么解决这个问题呢？</p>
<p>（1）先淘汰缓存<br>
（2）再写数据库（这两步和原来一样）<br>
（3）休眠1秒，再次淘汰缓存</p>
<p>这么做，可以将1秒内所造成的缓存脏数据，再次删除。</p>
<blockquote>
<p>针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。</p>
</blockquote>
<p>如果你用了mysql的读写分离架构怎么办？在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。</p>
<p>（1）请求A进行写操作，删除缓存<br>
（2）请求A将数据写入数据库了，<br>
（3）请求B查询缓存发现，缓存没有值<br>
（4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值<br>
（5）请求B将旧值写入缓存<br>
（6）数据库完成主从同步，从库变为新值</p>
<p>上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。</p>
<p>采用这种同步淘汰策略，吞吐量降低怎么办？那就将第二次删除作为<code>异步</code>的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。</p>
<p>所以在先删除缓存，再更新数据库的情况下，可以使用延时双删的策略，来保证脏数据只会存活一段时间，就会被准确的数据覆盖。</p>
<p>在先更新数据库，再删缓存的情况下，缓存出现脏数据的情况虽然可能性极小，但也会出现。我们依然可以用延时双删策略，在请求A对缓存写入了脏的旧值之后，再次删除缓存。来保证去掉脏缓存。</p>
<h5 id="删缓存失败了怎么办重试机制">删缓存失败了怎么办：重试机制</h5>
<p>看似问题都已经解决了，但其实，还有一个问题没有考虑到，那就是删除缓存的操作，失败了怎么办？比如延时双删的时候，第二次缓存删除失败了，那不还是没有清除脏数据吗？</p>
<p>方案一：<br>
（1）更新数据库数据；<br>
（2）缓存因为种种问题删除失败<br>
（3）将需要删除的key发送至消息队列<br>
（4）自己消费消息，获得需要删除的key<br>
（5）继续重试删除操作，直到成功</p>
<p>然而，该方案有一个缺点，对业务线代码造成大量的侵入。</p>
<p>方案二：<br>
（1）更新数据库数据<br>
（2）数据库会将操作信息写入binlog日志当中（而读取binlog的中间件，可以采用阿里开源的canal）<br>
（3）订阅程序提取出所需要的数据以及key<br>
（4）另起一段非业务代码，获得该信息<br>
（5）尝试删除缓存操作，发现删除失败<br>
（6）将这些信息发送至消息队列<br>
（7）重新从消息队列中获得该数据，重试操作。</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzU1NTA0NTEwMg==&amp;mid=2247484200&amp;idx=1&amp;sn=6b6c7251ee83fe8ef9201373aafcffdd&amp;chksm=fbdb1aa9ccac93bfe26655f89056b0d25b3a536f6b11148878fe96ffdf1d8349d44659cad784&amp;token=841068032&amp;lang=zh_CN#rd">🔗参考文档</a></p>
<h2 id="如何优雅的实现订单异步处理">如何优雅的实现订单异步处理</h2>
<h3 id="简单的订单异步处理实现">简单的订单异步处理实现</h3>
<p>在秒杀系统用户进行抢购的过程中，由于在同一时间会有大量请求涌入服务器，如果每个请求都立即访问数据库进行扣减库存+写入订单的操作，对数据库的压力是巨大的。</p>
<p>如何减轻数据库的压力呢，我们将每一条秒杀的请求存入消息队列（例如RabbitMQ）中，放入消息队列后，给用户返回类似“抢购请求发送成功”的结果。而在消息队列中，我们将收到的下订单请求一个个的写入数据库中，比起多线程同步修改数据库的操作，大大缓解了数据库的连接压力，最主要的好处就表现在数据库连接的减少：</p>
<ul>
<li>同步方式：大量请求快速占满数据库框架开启的数据库连接池，同时修改数据库，导致数据库读写性能骤减。</li>
<li>异步方式：一条条消息以顺序的方式写入数据库，连接数几乎不变（当然，也取决于消息队列消费者的数量</li>
</ul>
<p>这种实现可以理解为是一中流量削峰：让数据库按照他的处理能力，从消息队列中拿取消息进行处理。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1684477287392.png" alt="" loading="lazy"></figure>
<p>我们在源码仓库里，新增一个controller对外接口：</p>
<pre><code class="language-java">/**
 * 下单接口：异步处理订单
 * @param sid
 * @return
 */
@RequestMapping(value = &quot;/createUserOrderWithMq&quot;, method = {RequestMethod.GET})
@ResponseBody
public String createUserOrderWithMq(@RequestParam(value = &quot;sid&quot;) Integer sid,
                              @RequestParam(value = &quot;userId&quot;) Integer userId) {
    try {
        // 检查缓存中该用户是否已经下单过
        Boolean hasOrder = orderService.checkUserOrderInfoInCache(sid, userId);
        if (hasOrder != null &amp;&amp; hasOrder) {
            LOGGER.info(&quot;该用户已经抢购过&quot;);
            return &quot;你已经抢购过了，不要太贪心.....&quot;;
        }
        // 没有下单过，检查缓存中商品是否还有库存
        LOGGER.info(&quot;没有抢购过，检查缓存中商品是否还有库存&quot;);
        Integer count = stockService.getStockCount(sid);
        if (count == 0) {
            return &quot;秒杀请求失败，库存不足.....&quot;;
        }

        // 有库存，则将用户id和商品id封装为消息体传给消息队列处理
        // 注意这里的有库存和已经下单都是缓存中的结论，存在不可靠性，在消息队列中会查表再次验证
        LOGGER.info(&quot;有库存：[{}]&quot;, count);
        JSONObject jsonObject = new JSONObject();
        jsonObject.put(&quot;sid&quot;, sid);
        jsonObject.put(&quot;userId&quot;, userId);
        sendToOrderQueue(jsonObject.toJSONString());
        return &quot;秒杀请求提交成功&quot;;
    } catch (Exception e) {
        LOGGER.error(&quot;下单接口：异步处理订单异常：&quot;, e);
        return &quot;秒杀请求失败，服务器正忙.....&quot;;
    }
}
</code></pre>
<p>createUserOrderWithMq接口整体流程如下：</p>
<ul>
<li>检查缓存中该用户是否已经下单过：在消息队列下单成功后写入redis一条用户id和商品id绑定的数据</li>
<li>没有下单过，检查缓存中商品是否还有库存</li>
<li>缓存中如果有库存，则将用户id和商品id封装为消息体「传给消息队列处理」</li>
</ul>
<p>注意：这里的「有库存和已经下单」都是缓存中的结论，存在不可靠性，在消息队列中会查表再次验证，「作为兜底逻辑」</p>
<p>消息队列是如何接收消息的呢？我们新建一个消息队列，采用第四篇文中使用过的RabbitMQ，我再稍微贴一下整个创建RabbitMQ的流程把</p>
<pre><code class="language-java">&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<pre><code class="language-java">@Configuration
public class RabbitMqConfig {

    @Bean
    public Queue orderQueue() {
        return new Queue(&quot;orderQueue&quot;);
    }

}
</code></pre>
<p>添加一个消费者：</p>
<pre><code class="language-java">@Component
@RabbitListener(queues = &quot;orderQueue&quot;)
public class OrderMqReceiver {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderMqReceiver.class);

    @Autowired
    private StockService stockService;

    @Autowired
    private OrderService orderService;

    @RabbitHandler
    public void process(String message) {
        LOGGER.info(&quot;OrderMqReceiver收到消息开始用户下单流程: &quot; + message);
        JSONObject jsonObject = JSONObject.parseObject(message);
        try {
            orderService.createOrderByMq(jsonObject.getInteger(&quot;sid&quot;),jsonObject.getInteger(&quot;userId&quot;));
        } catch (Exception e) {
            LOGGER.error(&quot;消息处理异常：&quot;, e);
        }
    }
}
</code></pre>
<p>真正的下单的操作，在service中完成，我们在orderService中新建createOrderByMq方法：</p>
<pre><code class="language-java">@Override
public void createOrderByMq(Integer sid, Integer userId) throws Exception {

    Stock stock;
    //校验库存（不要学我在trycatch中做逻辑处理，这样是不优雅的。这里这样处理是为了兼容之前的秒杀系统文章）
    try {
        stock = checkStock(sid);
    } catch (Exception e) {
        LOGGER.info(&quot;库存不足！&quot;);
        return;
    }
    //乐观锁更新库存
    boolean updateStock = saleStockOptimistic(stock);
    if (!updateStock) {
        LOGGER.warn(&quot;扣减库存失败，库存已经为0&quot;);
        return;
    }

    LOGGER.info(&quot;扣减库存成功，剩余库存：[{}]&quot;, stock.getCount() - stock.getSale() - 1);
    stockService.delStockCountCache(sid);
    LOGGER.info(&quot;删除库存缓存&quot;);

    //创建订单
    LOGGER.info(&quot;写入订单至数据库&quot;);
    createOrderWithUserInfoInDB(stock, userId);
    LOGGER.info(&quot;写入订单至缓存供查询&quot;);
    createOrderWithUserInfoInCache(stock, userId);
    LOGGER.info(&quot;下单完成&quot;);

}
</code></pre>
<p>真正的下单的操作流程为：</p>
<ul>
<li>校验数据库库存</li>
<li>乐观锁更新库存（其他之前讲到的锁也可以啦）</li>
<li>写入订单至数据库</li>
<li>「写入订单和用户信息至缓存供查询」：写入后，在外层接口便可以通过判断redis中是否存在用户和商品的抢购信息，来直接给用户返回“你已经抢购过”的消息。</li>
</ul>
<p>我是如何在redis中记录商品和用户的关系的呢，我使用了set集合，key是商品id，而value则是用户id的集合，当然这样有一些不合理之处：</p>
<ul>
<li>这种结构默认了一个用户只能抢购一次这个商品</li>
<li>使用set集合，在用户过多后，每次检查需要遍历set，用户过多有性能问题</li>
</ul>
<pre><code class="language-java">@Override
    public Boolean checkUserOrderInfoInCache(Integer sid, Integer userId) throws Exception {
        String key = CacheKey.USER_HAS_ORDER.getKey() + &quot;_&quot; + sid;
        LOGGER.info(&quot;检查用户Id：[{}] 是否抢购过商品Id：[{}] 检查Key：[{}]&quot;, userId, sid, key);
        return stringRedisTemplate.opsForSet().isMember(key, userId.toString());
    }
</code></pre>
<h3 id="更加优雅的实现">更加优雅的实现</h3>
<p>我们实现了上面的异步处理后，用户那边得到的结果是怎么样的呢？</p>
<p>用户点击了提交订单，收到了消息：您的订单已经提交成功。然后用户啥也没看见，也没有订单号，用户开始慌了，点到了自己的个人中心——已付款。发现居然没有订单！（因为可能还在队列中处理）</p>
<p>这样的话，用户可能马上就要开始投诉了！太不人性化了，我们不能只为了开发方便，舍弃了用户体验！</p>
<p>所以我们要改进一下，如何改进呢？其实很简单：</p>
<ul>
<li>让前端在提交订单后，显示一个“排队中”</li>
<li>同时，前端不断请求 检查用户和商品是否已经有订单 的接口，如果得到订单已经处理完成的消息，页面跳转抢购成功。</li>
</ul>
<pre><code class="language-java">/**
 * 检查缓存中用户是否已经生成订单
 * @param sid
 * @return
 */
@RequestMapping(value = &quot;/checkOrderByUserIdInCache&quot;, method = {RequestMethod.GET})
@ResponseBody
public String checkOrderByUserIdInCache(@RequestParam(value = &quot;sid&quot;) Integer sid,
                              @RequestParam(value = &quot;userId&quot;) Integer userId) {
    // 检查缓存中该用户是否已经下单过
    try {
        Boolean hasOrder = orderService.checkUserOrderInfoInCache(sid, userId);
        if (hasOrder != null &amp;&amp; hasOrder) {
            return &quot;恭喜您，已经抢购成功！&quot;;
        }
    } catch (Exception e) {
        LOGGER.error(&quot;检查订单异常：&quot;, e);
    }
    return &quot;很抱歉，你的订单尚未生成，继续排队吧您嘞。&quot;;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[《Neo4j权威指南》一]]></title>
        <id>https://q456qq520.github.io/post/lesslessneo4j-quan-wei-zhi-nan-greatergreater-yi/</id>
        <link href="https://q456qq520.github.io/post/lesslessneo4j-quan-wei-zhi-nan-greatergreater-yi/">
        </link>
        <updated>2023-05-11T01:35:33.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="一-图数据库基础">一 图数据库基础</h2>
<h3 id="11-图数据模型">1.1 图数据模型</h3>
]]></summary>
        <content type="html"><![CDATA[<h2 id="一-图数据库基础">一 图数据库基础</h2>
<h3 id="11-图数据模型">1.1 图数据模型</h3>
<!-- more -->
<p>图数据要具体存储到图数据库中，最终落实为具体的数据文件，自然就涉及特定的图数据模型，即如何存，采用什么实现方式来存。常用的有三种：==属性图、超图和三元图。</p>
<p>其中Neo4j就采用属性图模型，因为属性图直观更易于理解，能描述大部分图使用场景。符合下列特征的图数据模型就称为属性图。</p>
<ul>
<li>它包含节点和关系</li>
<li>节点可以有属性（键值对）</li>
<li>节点可以有一个或多个标签</li>
<li>关系有名字和方向，并总是有一个开始节点和一个结束节点。</li>
<li>关系也可以有属性</li>
</ul>
<p>超图是一种更为广义对图模型，在超图中，一个关系（称作超边）可以关联任意数量的节点，无论是开始节点端还是结束节点端，而属性图中一个关系只允许一个开始节点和一个结束节点。因此，超图更适用表示多对多关系。</p>
<h3 id="12-图计算引擎">1.2 图计算引擎</h3>
<p>图数据库的核心也是构建在一个一起闹智商的，那就是图计算引擎，是能够组织存储大型图数据集并且实现了全局图计算算法的一种数据库核心构建。</p>
<p>它包含一个具有联机事务处理过程的数据库记录系统，图计算引擎用于响应用户终端允许时发来的查询请求，周期性从记录系统中进行数据抽取、转换和家在，然后将数据从记录数据系统读入到图计算引擎并进行离线查询和分析，最好将查询、分析的结果返回给用户终端。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1683787705759.png" alt="" loading="lazy"></figure>
<p>目前较为流程的图计算引擎有两种：单机图计算引擎和分布式图计算引擎。</p>
<h3 id="13-neo4j概述">1.3 Neo4j概述</h3>
<p>Neo4j是由Java实现的开源的NoSQL图数据库。</p>
<h3 id="14-neo4j底层存储结构">1.4 Neo4j底层存储结构</h3>
<p>免索引邻接是图数据库实现高效遍历的关键，那么免索引邻接的实现机制就是Neo4j底层存储结构设计的关键。能够支持高效的，本地化的图存储以及任意图算法的快速遍历，是使用图数据库的重要原因。</p>
<p>从宏观角度来说，Neo4j中仅仅只有两种数据类型：</p>
<ol>
<li>节点（Node）：节点类似E-R图中的实体，每一个实体可以有0个或多个属性，这些属性以key-value对的形式存在，属性没有特殊的类别要求，同时每个节点还具有相应的标签（Label），用来区分不同类型的节点。</li>
<li>关系（Relationship）：关系也类似与E-R图中的关系。一个挂你想有起始节点和终止节点。关系也有自己的属性和标签。</li>
</ol>
<p><img src="https://q456qq520.github.io/post-images/1683792894779.png" alt="" loading="lazy"><br>
节点和关系分别采用固定长度存储，节点存储文件用来存储节点的记录，文件名叫<code>neostore.nodestore.db</code>。节点记录的长度为固定大小，每个节点记录的长度为9字节。格式为：<code>Node:inUse+nextRelId+nextPropId</code>。</p>
<ul>
<li>inUse：1表示该节点被正常使用，0表示该节点被删除。</li>
<li>nextRelId：该节点的下一个关系ID</li>
<li>nextPropId：该节点的下一个属性ID</li>
</ul>
<p>如果有一个ID为100的节点，就能直接计算出该记录在存储文件中的第900个字节。成本仅为O(1)。</p>
<p>关系存储文件用来存储关系的记录，文件名为<code>neostore.relationshipstore.db</code>。像节点的存储一样，关系存储区的记录大小也是固定的，格式为<code>Relationshipstore:inUse+firstNode+secondNode+relType+firstPrevRelId+firstNextRelId+secondPrevRelId+secondNextRelId+nextPropId</code>。</p>
<ul>
<li>inUse，nextPropId：作用同上</li>
<li>firstNode：当前关系的起始节点</li>
<li>secondNode：当前关系的终止节点</li>
<li>relType：关系的类型</li>
<li>firstPrevRelId &amp; firstNextRelId：起始节点的前一个和后一个关系的ID</li>
<li>secondPrevRelId &amp; secondNextRelId+nextPropId：终止节点的前一个和后一个关系的ID</li>
</ul>
<p>Neo4j中有一个<code>.id</code>文件用来保持对未使用记录对跟踪，用来回收未使用的空间。节点和关系的存储文件只关系图的基本存储结构而不是属性数据。这两种记录都使用固定大小的记录，以便存储文件内的任何记录都可以根据ID快速的计算出来。</p>
<p>下图是Neo4j中其他常见的基本存储类型，属性记录的物理存储放置在<code>neostore.propertystore.db</code>文件中。与节点和关系的存储记录一样，属性的存储记录也是固定长度。每个属性记录包含4个属性块和属性链中下一个属性的ID。属性链是单向链表，而关系链是双向链表。一个属性记录中可以包含任何JVM支持的基本数据类型、字符串、基于类型的数组以及属性索引文件（<code>neostore.propertystore.db.index</code>）。属性索引文件主要用于存储属性的名称，属性索引的值部分存储的是指向动态内存的记录或者内联值，短字符串和短数组会直接内联在属性存储记录中。当长度超过属性记录中的propBlock长度限制之后，会单独存储在其他的动态存储文件中。</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1683795844065.png" alt="" loading="lazy"></figure>
<p>Neo4j中两种动态存储：动态字符串存储（<code>neostore.propertystore.db.strings</code>）和动态数组存储（<code>neostore.propertystore.db.arrays</code>）。动态存储记录是可以扩展的，如果一个属性长到一条动态存储记录仍然无法完全容纳时，可以申请多个动态存储记录逻辑上进行连接。</p>
<h3 id="15-neo4j的遍历方式">1.5 Neo4j的遍历方式</h3>
<p>每个节点记录都包含一个指向该节点的第一个属性的指针和联系链中第一个联系的指针。要读取一个节点的属性，从指向第一个属性的指针开始，遍历整个单向链表的结构。要找到一个节点的关系，从指向的第一个关系开始，遍历整个双向链表，知道找到。一单找到我们就可以与使用和超找节点属性一样的方法查找关系的属性。我们也可以很方便的获取起始节点和结束节点的ID，利用节点ID就可以立即得到每个节点在节点存储文件中的具体位置，时间复杂度为O(1)。</p>
<p>下面通过一个例子来讲解遍历关系和节点的详细过程，假如在Neo4j中纯粹来ABCDE5个节点和R1、R2、R3、R4、R5、R6、R7 7个关系，它们之间的关系如下图所示。<br>
<img src="https://q456qq520.github.io/post-images/1683859055787.png" alt="" loading="lazy"><br>
假如要遍历图中节点B的所有关系，只需要向<code>NODEB-NEXT</code>方向遍历，直到指向NULL为止。如下图所示，可以看出即节点B的所有关系为R1、R3、R4、R5。<br>
<img src="https://q456qq520.github.io/post-images/1683859485562.png" alt="" loading="lazy"></p>
<p>通过固定大小的存储记录和指针ID，只要跟随指针就可以简单的实现遍历并且告诉指向。要遍历一个节点到另一个节点的特定关系，在Neo4j中只需要遍历几个指针。</p>
<ul>
<li>从一个给定节点定位关系链中第一个关系的位置，可以通过计算它在关系存储的偏移量来获得。跟获得节点存储位置的方法一样，使用关系ID乘以关系记录的固定大小即可找到关系在存储文件中的正确位置。</li>
<li>在关系记录中，搜索第二个字段可以找到第二个节点的ID，用节点记录大小乘以节点ID可以得到节点在存储中的正确位置。</li>
</ul>
<h3 id="16-neo4j的存储优化">1.6 Neo4j的存储优化</h3>
<p>Neo4j支持存储优化（压缩和内联存储属性值），对于某些段字符的属性可以直接存储在属性文件中，而不是单独的放到另一个动态存储区，这样可以减少I/O操作并增加吞吐量。</p>
<p>Neo4j还可以对属性名称的空间严格维护。属性名称都通过属性索引文件从属性存储中间接引用。属性索引允许所有具有想用名称的属性共享单个记录，因而可以节省相当大的空间和I/O开销。</p>
<p>Neo4j采用缓存策略，保证那些经常访问的数据可以快速地被多次重复访问。Neo4j高速缓存的页面置换算法是基于最不经常使用的页置换（LFU）缓存策略，即使有些页面近期没有使用过，但是因为以前的使用频率很高，那么在短期之内它页不会被淘汰。</p>
<h2 id="二-neo4j-基础入门">二 Neo4j 基础入门</h2>
<h3 id="21-安装部署">2.1 安装部署</h3>
<p>步骤一：安装对应版本JDK<br>
步骤二：<a href="https://neo4j.com/" title="🔗官网下载">🔗官网下载</a></p>
<p>初始用户名、密码均为neo4j，安装好后，具体文件目录如下：<br>
<img src="https://q456qq520.github.io/post-images/1683871781471.png" alt="" loading="lazy"></p>
<p>其中bin目录为运行目录，下面为启动关闭命令：</p>
<pre><code class="language-shell">./neo4j  start

./neo4j stop
</code></pre>
<p>启动完毕后，本地访问地址默认为：http://localhost:7474/browser/</p>
<h3 id="22-neo4j-中基本元素与概念">2.2 Neo4j 中基本元素与概念</h3>
<h4 id="221-节点">2.2.1 节点</h4>
<p>节点（Node）是图数据库中的一个基本元素，用以表示一个实体记录，就像关系数据库中的一条记录一样。在Neo4j中节点可以包含多个属性和多个标签。</p>
<h4 id="222-关系">2.2.2 关系</h4>
<p>关系（Relationship）是图数据库中的一个基本元素。当数据库汇中已经存在节点后，需要将节点连接起来构成图。关系就是用来连接两个节点，也称为图论的边（Edge），其始端和末端都必须是节点，关系不能指向空也不能从空发起。关系和节点一样可以包含多个属性，但是关系只能有一个类型（Type）。</p>
<p>关系必须有开始节点和结束节点。两头都不能为空。节点可以被关系串联或并联起来，由于关系是有方向的，所以可以在由节点、关系组成的图中进行遍历操作。</p>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1683873968406.png" alt="" loading="lazy"></figure>
<p>在图的遍历操作中我们可以指定关系遍历的方向或者指定为无方向，因此在创建关系时不必为两个节点创建相互指向的关系，而是在遍历时不指定遍历方向即可。特别注意一个节点可以存在指向自己的关系。</p>
<h4 id="223-属性">2.2.3 属性</h4>
<p>属性是由键值对组成的，就像hash表一样，属性名类似变量名，属性值类似变量值。属性值可以是基本的数据类型，或者由基本数据类型组成的数组。</p>
<p><mark>属性值没有null的概念</mark>，如果一个属性不需要了可以直接将整个键值对都移除，在查询时，可以用<code>IS NULL</code>关键字判断属性是否存在。</p>
<table>
<thead>
<tr>
<th>类型</th>
<th style="text-align:center">说明</th>
<th style="text-align:right">取值范围</th>
</tr>
</thead>
<tbody>
<tr>
<td>boolean</td>
<td style="text-align:center">布尔值</td>
<td style="text-align:right">true/false</td>
</tr>
<tr>
<td>btye</td>
<td style="text-align:center">8位的整数</td>
<td style="text-align:right">-128～127，inclusive</td>
</tr>
<tr>
<td>short</td>
<td style="text-align:center">16位的整数</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>int</td>
<td style="text-align:center">32位的整数</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>long</td>
<td style="text-align:center">64位的整数</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>float</td>
<td style="text-align:center">32位的浮点数</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>double</td>
<td style="text-align:center">64位的浮点数</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>char</td>
<td style="text-align:center">16位的无符号整数代表的字符</td>
<td style="text-align:right"></td>
</tr>
<tr>
<td>string</td>
<td style="text-align:center">Unicode字符序列</td>
<td style="text-align:right"></td>
</tr>
</tbody>
</table>
<h4 id="224-路径">2.2.4 路径</h4>
<p>当使用节点和关系创建了一个图后，在此图中任意两个节点间都说可能存在路径的。路径也有长度的概念，也就是路径中关系的条数，单独一个节点也可以组成长度为0的路径。</p>
<h4 id="225-遍历traversal">2.2.5 遍历（Traversal）</h4>
<p>遍历的规则可以是广度优先。也可以是深度优先。</p>
<h3 id="23-官方实例入门">2.3 官方实例入门</h3>
<h4 id="231-创建图数据">2.3.1 创建图数据</h4>
<p>首先创建一个电影节点，节点上有三个属性，分别代表电影标题、发布时间、宣传词。</p>
<pre><code class="language-sql">CREATE (TheMatrix:Movie {title:'The Matrix', released:1999, tagline:'Welcome to the Real World'})
</code></pre>
<p>再创建人物节点，节点有两个属性，名字和出生日期。</p>
<pre><code class="language-sql">CREATE (Keanu:Person {name:'Keanu Reeves', born:1964})
CREATE (Carrie:Person {name:'Carrie-Anne Moss', born:1967})
CREATE (Laurence:Person {name:'Laurence Fishburne', born:1961})
CREATE (Hugo:Person {name:'Hugo Weaving', born:1960})
CREATE (LillyW:Person {name:'Lilly Wachowski', born:1967})
CREATE (LanaW:Person {name:'Lana Wachowski', born:1965})
CREATE (JoelS:Person {name:'Joel Silver', born:1952})
</code></pre>
<p>然后创建演员、导演关系。</p>
<pre><code class="language-sql">  CREATE
      (Keanu)-[:ACTED_IN {roles:['Neo']}]-&gt;(TheMatrix),
      (Carrie)-[:ACTED_IN {roles:['Trinity']}]-&gt;(TheMatrix),
      (Laurence)-[:ACTED_IN {roles:['Morpheus']}]-&gt;(TheMatrix),
      (Hugo)-[:ACTED_IN {roles:['Agent Smith']}]-&gt;(TheMatrix),
      (LillyW)-[:DIRECTED]-&gt;(TheMatrix),
      (LanaW)-[:DIRECTED]-&gt;(TheMatrix),
      (JoelS)-[:PRODUCED]-&gt;(TheMatrix)
</code></pre>
<p>其中使用到了箭头运算符，如：(Keanu)-[:ACTED_IN {roles:['Neo']}]-&gt;(TheMatrix)，代表创建一个演员参演电影的关系，演员Keanu以角色roles:['Neo']参演ACTED_IN到电影中。  (LillyW)-[:DIRECTED]-&gt;(TheMatrix)表示导演与电影的关系。执行完毕后可以看到如下所示：<br>
<img src="https://q456qq520.github.io/post-images/1683883351323.png" alt="" loading="lazy"></p>
<h4 id="231-检索节点">2.3.1 检索节点</h4>
<h5 id="查找人员">查找人员</h5>
<ol>
<li>
<p>查找名为Tom Hanks的人物<br>
MATCH (tom {name:&quot;Tom Hanks&quot;}) RETURN tom</p>
</li>
<li>
<p>查找名为Cloud Atlas的电影<br>
MATCH (cloudAtlas {name:&quot;Cloud Atlas&quot;}) RETURN tom</p>
</li>
<li>
<p>随机查找10个人物的人名<br>
MATCH (people:Person) RETURN people.name LIMIT 10</p>
</li>
<li>
<p>查找多个电影，1990-2000发行的电影的名称<br>
MATCH (nineties:Movie) WHERE nineties.released &gt; 1990 AND nineties.released &lt; 2000 RETURN nineties.title</p>
</li>
</ol>
<h4 id="232-查询关系">2.3.2 查询关系</h4>
<ol>
<li>
<p>查找Tom Hanks参演过的电影的名称<br>
MATCH (tom:Person {name:&quot;Tom Hanks&quot;}) - [:ACTED_IN] -&gt; (tomHanksMovies) RETURN tom,tomHanksMovies</p>
</li>
<li>
<p>查询谁导演了电影“Cloud Atlas”<br>
MATCH (cloudAtlas {title:&quot;Cloud Atlas&quot;}) &lt;- [:DIRECTED] - (directors) RETURN directors.name<br>
首先匹配属性{title:&quot;Cloud Atlas&quot;}的节点，然后匹配此节点具有关系 [:DIRECTED] 并且是被某节点指向的节点，再返回name属性。</p>
</li>
<li>
<p>查找与Tom Hanks同出演过电影的人<br>
MATCH (tom:Person {name:&quot;Tom Hanks&quot;}) - [:ACTED_IN] -&gt; (tomHanksMovies) &lt;- [:ACTED_IN] - (coActors) RETURN coActors.name<br>
首先匹配节点类型为Person，属性为tom hanks的节点，然后匹配此节点通过 [:ACTED_IN] 关系指向的节点，并且同时匹配该节点的 [:ACTED_IN] 关系节点。</p>
</li>
<li>
<p>查询与电影“Cloud Atlas”相关的所有人<br>
MATCH (people:Person) - [relatedTo] - (:Movie {title: &quot;Cloud Atlas&quot;}) RETURN people.name, Type(relatedTo),relatedTo<br>
首先匹配节点类型为Person的节点，然后匹配节点类型为Movie、节点属性为{title: &quot;Cloud Atlas&quot;}的节点，最好匹配他们两者之间存在某种关系，最后返回。</p>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[基于代价的慢查询优化建议]]></title>
        <id>https://q456qq520.github.io/post/ji-yu-dai-jie-de-man-cha-xun-you-hua-jian-yi/</id>
        <link href="https://q456qq520.github.io/post/ji-yu-dai-jie-de-man-cha-xun-you-hua-jian-yi/">
        </link>
        <updated>2023-04-21T07:13:41.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="1-背景">1 背景</h2>
<p>慢查询是指数据库中查询时间超过指定阈值的SQL，它是数据库的性能杀手，也是业务优化数据库访问的重要抓手。随着美团业务的高速增长，日均慢查询量已经过亿条，此前因慢查询导致的故障约占数据库故障总数的10%以上，而且高级别的故障呈日益增长趋势。因此，对慢查询的优化已经变得刻不容缓。</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="1-背景">1 背景</h2>
<p>慢查询是指数据库中查询时间超过指定阈值的SQL，它是数据库的性能杀手，也是业务优化数据库访问的重要抓手。随着美团业务的高速增长，日均慢查询量已经过亿条，此前因慢查询导致的故障约占数据库故障总数的10%以上，而且高级别的故障呈日益增长趋势。因此，对慢查询的优化已经变得刻不容缓。</p>
<!-- more -->
<p>那么如何优化慢查询呢？最直接有效的方法就是选用一个查询效率高的索引。关于高效率的索引推荐，主要在日常工作中，基于经验规则的推荐随处可见，对于简单的SQL，如<mark>select * from sync_test1 where name like 'Bobby%'</mark>，直接添加索引IX(name) 就可以取得不错的效果；但对于稍微复杂点的SQL，如<mark>select * from sync_test1 where name like 'Bobby%' and dt &gt; '2021-07-06'</mark>，到底选择IX(name)、IX(dt)、IX(dt,name) 还是IX(name,dt)，该方法也无法给出准确的回答。更别说像多表Join、子查询这样复杂的场景了。所以采用基于代价的推荐来解决该问题会更加普适，因为基于代价的方法使用了和数据库优化器相同的方式，去量化评估所有的可能性，选出的是执行SQL耗费代价最小的索引。</p>
<h2 id="2-基于代价的优化器介绍">2 基于代价的优化器介绍</h2>
<h3 id="21-sql执行与优化器">2.1 SQL执行与优化器</h3>
<p>一条SQL在MySQL服务器中执行流程主要包含：SQL解析、基于语法树的准备工作、优化器的逻辑变化、优化器的代价准备工作、基于代价模型的优化、进行额外的优化和运行执行计划等部分。具体如下图所示：</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1682064829235.jpeg" alt="" loading="lazy"></figure>
<h3 id="22-代价模型介绍">2.2 代价模型介绍</h3>
<p>而对于优化器来说，执行一条SQL有各种各样的方案可供选择，如表是否用索引、选择哪个索引、是否使用范围扫描、多表Join的连接顺序和子查询的执行方式等。如何从这些可选方案中选出耗时最短的方案呢？这就需要定义一个量化数值指标，这个指标就是代价(Cost)，我们分别计算出可选方案的操作耗时，从中选出最小值。</p>
<p>代价模型将操作分为Server层和Engine（存储引擎）层两类，<mark>Server层主要是CPU代价，Engine层主要是IO代价</mark>，比如MySQL从磁盘读取一个数据页的代价io_block_read_cost为1，计算符合条件的行代价为row_evaluate_cost为0.2。除此之外还有：</p>
<ol>
<li>memory_temptable_create_cost (default 2.0) 内存临时表的创建代价。</li>
<li>memory_temptable_row_cost (default 0.2) 内存临时表的行代价。</li>
<li>key_compare_cost (default 0.1) 键比较的代价，例如排序。</li>
<li>disk_temptable_create_cost (default 40.0) 内部myisam或innodb临时表的创建代价。</li>
<li>disk_temptable_row_cost (default 1.0) 内部myisam或innodb临时表的行代价。</li>
</ol>
<p>在MySQL 5.7中，这些操作代价的默认值都可以进行配置。为了计算出方案的总代价，还需要参考一些统计数据，如表数据量大小、元数据和索引信息等。MySQL的代价优化器模型整体如下图所示：</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1682065265190.jpeg" alt="" loading="lazy"></figure>
<h3 id="23-基于代价的索引选择">2.3 基于代价的索引选择</h3>
<p>还是继续拿上述的SQL select * from sync_test1 where name like 'Bobby%' and dt &gt; '2021-07-06'为例，我们看看MySQL优化器是如何根据代价模型选择索引的。首先，我们直接在建表时加入四个候选索引。</p>
<pre><code class="language-mysql">Create Table: CREATE TABLE `sync_test1` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `cid` int(11) NOT NULL,
    `phone` int(11) NOT NULL,
    `name` varchar(10) NOT NULL,
    `address` varchar(255) DEFAULT NULL,
    `dt` datetime DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `IX_name` (`name`),
    KEY `IX_dt` (`dt`),
    KEY `IX_dt_name` (`dt`,`name`),
    KEY `IX_name_dt` (`name`,`dt`)
    ) ENGINE=InnoDB
</code></pre>
<p>通过执行explain看出MySQL最终选择了IX_name索引。</p>
<pre><code class="language-mysql">mysql&gt; explain  select * from sync_test1 where name like 'Bobby%' and dt &gt; '2021-07-06';
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
| id | select_type | table      | partitions | type  | possible_keys                       | key     | key_len | ref  | rows | filtered | Extra                              |
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
|  1 | SIMPLE      | sync_test1 | NULL       | range | IX_name,IX_dt,IX_dt_name,IX_name_dt | IX_name | 12      | NULL |  572 |    36.83 | Using index condition; Using where |
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
</code></pre>
<p>然后再打开MySQL追踪优化器Trace功能。可以看出，没有选择其他三个索引的原因均是因为在其他三个索引上使用range scan的代价均&gt;= IX_name。</p>
<pre><code class="language-mysql">mysql&gt; select * from INFORMATION_SCHEMA.OPTIMIZER_TRACE\G;
*************************** 1. row ***************************

TRACE: {
...
&quot;rows_estimation&quot;: [
{
&quot;table&quot;: &quot;`sync_test1`&quot;,
&quot;range_analysis&quot;: {
&quot;table_scan&quot;: {
  &quot;rows&quot;: 105084,
  &quot;cost&quot;: 21628
},
...
&quot;analyzing_range_alternatives&quot;: {
  &quot;range_scan_alternatives&quot;: [
    {
      &quot;index&quot;: &quot;IX_name&quot;,
      &quot;ranges&quot;: [
        &quot;Bobby\u0000\u0000\u0000\u0000\u0000 &lt;= name &lt;= Bobbyÿÿÿÿÿ&quot;
      ],
      &quot;index_dives_for_eq_ranges&quot;: true,
      &quot;rowid_ordered&quot;: false,
      &quot;using_mrr&quot;: false,
      &quot;index_only&quot;: false,
      &quot;rows&quot;: 572,
      &quot;cost&quot;: 687.41,
      &quot;chosen&quot;: true
    },
    {
      &quot;index&quot;: &quot;IX_dt&quot;,
      &quot;ranges&quot;: [
        &quot;0x99aa0c0000 &lt; dt&quot;
      ],
      &quot;index_dives_for_eq_ranges&quot;: true,
      &quot;rowid_ordered&quot;: false,
      &quot;using_mrr&quot;: false,
      &quot;index_only&quot;: false,
      &quot;rows&quot;: 38698,
      &quot;cost&quot;: 46439,
      &quot;chosen&quot;: false,
      &quot;cause&quot;: &quot;cost&quot;
    },
    {
      &quot;index&quot;: &quot;IX_dt_name&quot;,
      &quot;ranges&quot;: [
        &quot;0x99aa0c0000 &lt; dt&quot;
      ],
      &quot;index_dives_for_eq_ranges&quot;: true,
      &quot;rowid_ordered&quot;: false,
      &quot;using_mrr&quot;: false,
      &quot;index_only&quot;: false,
      &quot;rows&quot;: 38292,
      &quot;cost&quot;: 45951,
      &quot;chosen&quot;: false,
      &quot;cause&quot;: &quot;cost&quot;
    },
    {
      &quot;index&quot;: &quot;IX_name_dt&quot;,
      &quot;ranges&quot;: [
        &quot;Bobby\u0000\u0000\u0000\u0000\u0000 &lt;= name &lt;= Bobbyÿÿÿÿÿ&quot;
      ],
      &quot;index_dives_for_eq_ranges&quot;: true,
      &quot;rowid_ordered&quot;: false,
      &quot;using_mrr&quot;: false,
      &quot;index_only&quot;: false,
      &quot;rows&quot;: 572,
      &quot;cost&quot;: 687.41,
      &quot;chosen&quot;: false,
      &quot;cause&quot;: &quot;cost&quot;
    }
  ],
  &quot;analyzing_roworder_intersect&quot;: {
    &quot;usable&quot;: false,
    &quot;cause&quot;: &quot;too_few_roworder_scans&quot;
  }
},
&quot;chosen_range_access_summary&quot;: {
  &quot;range_access_plan&quot;: {
    &quot;type&quot;: &quot;range_scan&quot;,
    &quot;index&quot;: &quot;IX_name&quot;,
    &quot;rows&quot;: 572,
    &quot;ranges&quot;: [
      &quot;Bobby\u0000\u0000\u0000\u0000\u0000 &lt;= name &lt;= Bobbyÿÿÿÿÿ&quot;
    ]
  },
  &quot;rows_for_plan&quot;: 572,
  &quot;cost_for_plan&quot;: 687.41,
  &quot;chosen&quot;: true
}
...
}
</code></pre>
<p>下面我们根据代价模型来推演一下代价的计算过程：</p>
<ol>
<li>走全表扫描的代价：io_cost + cpu_cost = （数据页个数 * io_block_read_cost）+ (数据行数 * row_evaluate_cost + 1.1) = （data_length / block_size + 1）+ (rows * 0.2 + 1.1) = (9977856 / 16384 + 1) + (105084 * 0.2 + 1.1) = 21627.9。</li>
<li>走二级索引IX_name的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01) = (572 * 1 + 1) + (572*0.2 + 0.01) = 687.41。</li>
<li>走二级索引IX_dt的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01) = (38698 * 1 + 1) + (38698*0.2 + 0.01) = 46438.61。</li>
<li>走二级索引IX_dt_name的代价: io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01) = (38292 * 1 + 1) + (38292 * 0.2 + 0.01) = 45951.41。</li>
<li>走二级索引IX_name_dt的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01) = (572 * 1 + 1) + (572*0.2 + 0.01) = 687.41。</li>
</ol>
<p>补充说明</p>
<ol>
<li>计算结果在小数上有偏差，因为MySQL使用%g打印浮点数，小数会以最短的方式输出。</li>
<li>除“+1.1 +1”这种调节值外，Cost计算还会出现+0.01, 它是为了避免index scan和range scan出现Cost的竞争。</li>
<li>Cost计算是基于MySQL的默认参数配置，如果Cost Model参数改变，optimizer_switch的选项不同，数据分布不同都会导致最终Cost的计算结果不同。</li>
<li>data_length可查询information_schema.tables，block_size默认16K。</li>
</ol>
<h3 id="24-基于代价的索引推荐思路">2.4 基于代价的索引推荐思路</h3>
<p>如果想借助MySQL优化器给慢查询计算出最佳索引，那么需要真实地在业务表上添加所有候选索引。对于线上业务来说，直接添加索引的时间空间成本太高，是不可接受的。MySQL优化器选最佳索引用到的数据是索引元数据和统计数据，所以我们想是否可以通过给它提供候选索引的这些数据，而非真实添加索引的这种方式来实现。</p>
<p>通过深入调研MySQL的代码结构和优化器流程，我们发现是可行的：一部分存在于Server层的frm文件中，比如索引定义；另一部分存在于Engine层中，或者通过调用Engine层的接口函数来获取，比如索引中某个列的不同值个数、索引占据的页面大小等。索引相关的信息，如下图所示：</p>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1682067083614.jpeg" alt="" loading="lazy"></figure>
<p>因为MySQL本身就支持自定义存储引擎，所以索引推荐思路是构建一个支持虚假索引的存储引擎，在它上面建立包含候选索引的空表，再采集样本数据，计算出统计数据提供给优化器，让优化器选出最优索引，整个调用关系如下图所示：</p>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1682067095473.jpeg" alt="" loading="lazy"></figure>
<h2 id="3-索引推荐实现">3 索引推荐实现</h2>
<p>因为存储引擎本身并不具备对外提供服务的能力，直接在MySQL Server层修改也难以维护，所以我们将整个索引推荐系统拆分成支持虚假索引的Fakeindex存储引擎和对外提供服务的Go-Server两部分，整体架构图如下：</p>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1682067368398.jpeg" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java魔法类：Unsafe应用解析]]></title>
        <id>https://q456qq520.github.io/post/java-mo-fa-lei-unsafe-ying-yong-jie-xi/</id>
        <link href="https://q456qq520.github.io/post/java-mo-fa-lei-unsafe-ying-yong-jie-xi/">
        </link>
        <updated>2023-04-21T06:24:33.000Z</updated>
        <summary type="html"><![CDATA[<p>Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如<code>直接访问系统内存资源</code>、<code>自主管理内存资源</code>等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。</p>
]]></summary>
        <content type="html"><![CDATA[<p>Unsafe是位于sun.misc包下的一个类，主要提供一些用于执行低级别、不安全操作的方法，如<code>直接访问系统内存资源</code>、<code>自主管理内存资源</code>等，这些方法在提升Java运行效率、增强Java语言底层资源操作能力方面起到了很大的作用。但由于Unsafe类使Java语言拥有了类似C语言指针一样操作内存空间的能力，这无疑也增加了程序发生相关指针问题的风险。在程序中过度、不正确使用Unsafe类会使得程序出错的概率变大，使得Java这种安全的语言变得不再“安全”，因此对Unsafe的使用一定要慎重。</p>
<!-- more -->
<p>如下Unsafe源码所示，Unsafe类为一单例实现，提供静态方法getUnsafe获取Unsafe实例，当且仅当调用getUnsafe方法的类为引导类加载器所加载时才合法，否则抛出SecurityException异常。</p>
<pre><code class="language-java">public final class Unsafe {
  // 单例对象
  private static final Unsafe theUnsafe;

  private Unsafe() {
  }
  @CallerSensitive
  public static Unsafe getUnsafe() {
    Class var0 = Reflection.getCallerClass();
    // 仅在引导类加载器`BootstrapClassLoader`加载时才合法
    if(!VM.isSystemDomainLoader(var0.getClassLoader())) {    
      throw new SecurityException(&quot;Unsafe&quot;);
    } else {
      return theUnsafe;
    }
  }
}
</code></pre>
<p>那如若想使用这个类，该如何获取其实例？有如下两个可行方案。</p>
<p>其一，从getUnsafe方法的使用限制条件出发，通过Java命令行命令-Xbootclasspath/a把调用Unsafe相关方法的类A所在jar包路径追加到默认的bootstrap路径中，使得A被引导类加载器加载，从而通过Unsafe.getUnsafe方法安全的获取Unsafe实例。</p>
<blockquote>
<p>java -Xbootclasspath/a: ${path}   // 其中path为调用Unsafe相关方法的类所在jar包路径</p>
</blockquote>
<p>其二，通过反射获取单例对象theUnsafe。</p>
<pre><code class="language-java">private static Unsafe reflectGetUnsafe() {
    try {
      Field field = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);
      field.setAccessible(true);
      return (Unsafe) field.get(null);
    } catch (Exception e) {
      log.error(e.getMessage(), e);
      return null;
    }
}
</code></pre>
<h2 id="功能介绍">功能介绍</h2>
<p><img src="https://q456qq520.github.io/post-images/1682058668352.png" alt="" loading="lazy"><br>
如上图所示，Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类。</p>
<h2 id="内存操作">内存操作</h2>
<p>这部分主要包含堆外内存的分配、拷贝、释放、给定地址值操作等方法。</p>
<pre><code class="language-java">//分配内存, 相当于C++的malloc函数
public native long allocateMemory(long bytes);
//扩充内存
public native long reallocateMemory(long address, long bytes);
//释放内存
public native void freeMemory(long address);
//在给定的内存块中设置值
public native void setMemory(Object o, long offset, long bytes, byte value);
//内存拷贝
public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
//获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar等
public native Object getObject(Object o, long offset);
//为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar等
public native void putObject(Object o, long offset, Object x);
//获取给定地址的byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果为确定的）
public native byte getByte(long address);
//为给定地址设置byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果才是确定的）
public native void putByte(long address, byte x);
</code></pre>
<p>通常，我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。</p>
<h3 id="使用堆外内存的原因">使用堆外内存的原因</h3>
<ul>
<li>对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在GC时减少回收停顿对于应用的影响。</li>
<li>提升程序I/O操作的性能。通常在I/O通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。</li>
</ul>
<h3 id="典型应用">典型应用</h3>
<p><code>DirectByteBuffer</code>是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。</p>
<p>下图为DirectByteBuffer构造函数，创建DirectByteBuffer的时候，通过Unsafe.allocateMemory分配内存、Unsafe.setMemory进行内存初始化，而后构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，分配的堆外内存一起被释放。</p>
<figure data-type="image" tabindex="1"><img src="https://q456qq520.github.io/post-images/1682059248322.png" alt="" loading="lazy"></figure>
<p>那么如何通过构建垃圾回收追踪对象Cleaner实现堆外内存释放呢？</p>
<p>Cleaner继承自Java四大引用类型之一的虚引用<code>PhantomReference</code>（众所周知，无法通过虚引用获取与之关联的对象实例，且当对象仅被虚引用引用时，在任何发生GC的时候，其均可被回收），通常PhantomReference与引用队列ReferenceQueue结合使用，可以实现虚引用关联对象被垃圾回收时能够进行系统通知、资源清理等功能。如下图所示，当某个被Cleaner引用的对象将被回收时，JVM垃圾收集器会将此对象的引用放入到对象引用中的pending链表中，等待Reference-Handler进行相关处理。其中，Reference-Handler为一个拥有最高优先级的守护线程，会循环不断的处理pending链表中的对象引用，执行Cleaner的clean方法进行相关清理工作。</p>
<figure data-type="image" tabindex="2"><img src="https://q456qq520.github.io/post-images/1682059416664.png" alt="" loading="lazy"></figure>
<p>所以当DirectByteBuffer仅被Cleaner引用（即为虚引用）时，其可以在任意GC时段被回收。当DirectByteBuffer实例对象被回收时，在Reference-Handler线程操作中，会调用Cleaner的clean方法根据创建Cleaner时传入的Deallocator来进行堆外内存的释放。</p>
<figure data-type="image" tabindex="3"><img src="https://q456qq520.github.io/post-images/1682059508405.png" alt="" loading="lazy"></figure>
<h2 id="cas相关">CAS相关</h2>
<p>如下源代码释义所示，这部分主要为CAS相关操作的方法。</p>
<pre><code class="language-java">/**
	*  CAS
  * @param o         包含要修改field的对象
  * @param offset    对象中某field的偏移量
  * @param expected  期望值
  * @param update    更新值
  * @return          true | false
  */
public final native boolean compareAndSwapObject(Object o, long offset,  Object expected, Object update);

public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);
  
public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);
</code></pre>
<p>什么是CAS? 即比较并替换，实现并发算法时常用到的一种技术。CAS操作包含三个操作数——<mark>内存位置、预期原值及新值</mark>。执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS是一条CPU的原子指令（<code>cmpxchg指令</code>），不会造成所谓的数据不一致问题，Unsafe提供的CAS方法（如compareAndSwapXXX）底层实现即为CPU指令cmpxchg。</p>
<h3 id="典型应用-2">典型应用</h3>
<p>CAS在java.util.concurrent.atomic相关类、Java AQS、CurrentHashMap等实现上有非常广泛的应用。如下图所示，AtomicInteger的实现中，静态字段valueOffset即为字段value的内存偏移地址，valueOffset的值在AtomicInteger初始化时，在静态代码块中通过Unsafe的objectFieldOffset方法获取。在AtomicInteger中提供的线程安全方法中，通过字段valueOffset的值可以定位到AtomicInteger对象中value的内存地址，从而可以根据CAS实现对value字段的原子操作。</p>
<figure data-type="image" tabindex="4"><img src="https://q456qq520.github.io/post-images/1682059730822.png" alt="" loading="lazy"></figure>
<p>下图为某个AtomicInteger对象自增操作前后的内存示意图，对象的基地址baseAddress=“0x110000”，通过baseAddress+valueOffset得到value的内存地址valueAddress=“0x11000c”；然后通过CAS进行原子性的更新操作，成功则返回，否则继续重试，直到更新成功为止。</p>
<figure data-type="image" tabindex="5"><img src="https://q456qq520.github.io/post-images/1682059759899.png" alt="" loading="lazy"></figure>
<h2 id="线程调度">线程调度</h2>
<p>这部分，包括线程挂起、恢复、锁机制等方法。</p>
<pre><code class="language-java">//取消阻塞线程
public native void unpark(Object thread);
//阻塞线程
public native void park(boolean isAbsolute, long time);
//获得对象锁（可重入锁）
@Deprecated
public native void monitorEnter(Object o);
//释放对象锁
@Deprecated
public native void monitorExit(Object o);
//尝试获取对象锁
@Deprecated
public native boolean tryMonitorEnter(Object o);
</code></pre>
<p>如上源码说明中，方法park、unpark即可实现线程的挂起与恢复，将一个线程进行挂起是通过park方法实现的，调用park方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark可以终止一个挂起的线程，使其恢复正常。</p>
<h3 id="典型应用-3">典型应用</h3>
<p>Java锁和同步器框架的核心类AbstractQueuedSynchronizer，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的，而LockSupport的park、unpark方法实际是调用Unsafe的park、unpark方式来实现。</p>
<h2 id="class相关">Class相关</h2>
<p>此部分主要提供Class和它的静态字段的操作相关方法，包含静态字段内存定位、定义类、定义匿名类、检验&amp;确保初始化等。</p>
<pre><code class="language-java">//获取给定静态字段的内存地址偏移量，这个值对于给定的字段是唯一且固定不变的
public native long staticFieldOffset(Field f);
//获取一个静态类中给定字段的对象指针
public native Object staticFieldBase(Field f);
//判断是否需要初始化一个类，通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 当且仅当ensureClassInitialized方法不生效时返回false。
public native boolean shouldBeInitialized(Class&lt;?&gt; c);
//检测给定的类是否已经初始化。通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。
public native void ensureClassInitialized(Class&lt;?&gt; c);
//定义一个类，此方法会跳过JVM的所有安全检查，默认情况下，ClassLoader（类加载器）和ProtectionDomain（保护域）实例来源于调用者
public native Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain);
//定义一个匿名类
public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; hostClass, byte[] data, Object[] cpPatches);
</code></pre>
<h3 id="典型应用-4">典型应用</h3>
<p>从Java 8开始，JDK使用invokedynamic及VM Anonymous Class结合来实现Java语言层面上的Lambda表达式。</p>
<ul>
<li>invokedynamic： invokedynamic是Java 7为了实现在JVM上运行动态语言而引入的一条新的虚拟机指令，它可以实现在运行期动态解析出调用点限定符所引用的方法，然后再执行该方法，invokedynamic指令的分派逻辑是由用户设定的引导方法决定。</li>
<li>VM Anonymous Class：可以看做是一种模板机制，针对于程序动态生成很多结构相同、仅若干常量不同的类时，可以先创建包含常量占位符的模板类，而后通过Unsafe.defineAnonymousClass方法定义具体类时填充模板的占位符生成具体的匿名类。生成的匿名类不显式挂在任何ClassLoader下面，只要当该类没有存在的实例对象、且没有强引用来引用该类的Class对象时，该类就会被GC回收。故而VM Anonymous Class相比于Java语言层面的匿名内部类无需通过ClassClassLoader进行类加载且更易回收。</li>
</ul>
<p>在Lambda表达式实现中，通过invokedynamic指令调用引导方法生成调用点，在此过程中，会通过ASM动态生成字节码，而后利用Unsafe的defineAnonymousClass方法定义实现相应的函数式接口的匿名类，然后再实例化此匿名类，并返回与此匿名类中函数式方法的方法句柄关联的调用点；而后可以通过此调用点实现调用相应Lambda表达式定义逻辑的功能。</p>
<h2 id="对象操作">对象操作</h2>
<p>此部分主要包含对象成员属性相关操作及非常规的对象实例化方式等相关方法。</p>
<pre><code class="language-java">//返回对象成员属性在内存地址相对于此对象的内存地址的偏移量
public native long objectFieldOffset(Field f);
//获得给定对象的指定地址偏移量的值，与此类似操作还有：getInt，getDouble，getLong，getChar等
public native Object getObject(Object o, long offset);
//给定对象的指定地址偏移量设值，与此类似操作还有：putInt，putDouble，putLong，putChar等
public native void putObject(Object o, long offset, Object x);
//从对象的指定偏移量处获取变量的引用，使用volatile的加载语义
public native Object getObjectVolatile(Object o, long offset);
//存储变量的引用到对象的指定的偏移量处，使用volatile的存储语义
public native void putObjectVolatile(Object o, long offset, Object x);
//有序、延迟版本的putObjectVolatile方法，不保证值的改变被其他线程立即看到。只有在field被volatile修饰符修饰时有效
public native void putOrderedObject(Object o, long offset, Object x);
//绕过构造方法、初始化代码来创建对象
public native Object allocateInstance(Class&lt;?&gt; cls) throws InstantiationException;
</code></pre>
<p>###典型应用</p>
<ul>
<li>常规对象实例化方式：我们通常所用到的创建对象的方式，从本质上来讲，都是通过new机制来实现对象的创建。但是，new机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。</li>
<li>非常规的实例化方式：而Unsafe中提供allocateInstance方法，仅通过Class对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM安全检查等。它抑制修饰符检测，也就是即使构造器是private修饰的也能通过此方法实例化，只需提类对象即可创建相应的对象。由于这种特性，allocateInstance在java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应的应用。</li>
</ul>
<h2 id="数组相关">数组相关</h2>
<p>这部分主要介绍与数据操作相关的arrayBaseOffset与arrayIndexScale这两个方法，两者配合起来使用，即可定位数组中每个元素在内存中的位置。</p>
<pre><code class="language-java">//返回数组中第一个元素的偏移地址
public native int arrayBaseOffset(Class&lt;?&gt; arrayClass);
//返回数组中一个元素占用的大小
public native int arrayIndexScale(Class&lt;?&gt; arrayClass);
</code></pre>
<h3 id="典型应用-5">典型应用</h3>
<p>这两个与数据操作相关的方法，在java.util.concurrent.atomic 包下的AtomicIntegerArray（可以实现对Integer数组中每个元素的原子性操作）中有典型的应用，如下图AtomicIntegerArray源码所示，通过Unsafe的arrayBaseOffset、arrayIndexScale分别获取数组首元素的偏移地址base及单个元素大小因子scale。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图二所示的getAndAdd方法即通过checkedByteOffset方法获取某数组元素的偏移地址，而后通过CAS实现原子性操作。</p>
<figure data-type="image" tabindex="6"><img src="https://q456qq520.github.io/post-images/1682060445869.png" alt="" loading="lazy"></figure>
<h2 id="内存屏障">内存屏障</h2>
<p>在Java 8中引入，用于定义内存屏障（也称内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，是CPU或编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作），避免代码重排序。</p>
<pre><code class="language-java">//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前
public native void loadFence();
//内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前
public native void storeFence();
//内存屏障，禁止load、store操作重排序
public native void fullFence();
</code></pre>
<p>###典型应用<br>
在Java 8中引入了一种锁的新机制——<code>StampedLock</code>，它可以看成是读写锁的一个改进版本。StampedLock提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到线程工作内存时，会存在数据不一致问题，所以当使用StampedLock的乐观读锁时，需要遵从如下图用例中使用的模式来确保数据的一致性。<br>
<img src="https://q456qq520.github.io/post-images/1682060534549.png" alt="" loading="lazy"></p>
<p>如上图用例所示计算坐标点Point对象，包含点移动方法move及计算此点到原点的距离的方法distanceFromOrigin。在方法distanceFromOrigin中，首先，通过tryOptimisticRead方法获取乐观读标记；然后从主内存中加载点的坐标值 (x,y)；而后通过StampedLock的validate方法校验锁状态，判断坐标点(x,y)从主内存加载到线程工作内存过程中，主内存的值是否已被其他线程通过move方法修改，如果validate返回值为true，证明(x, y)的值未被修改，可参与后续计算；否则，需加悲观读锁，再次从主内存加载(x,y)的最新值，然后再进行距离计算。其中，校验锁状态这步操作至关重要，需要判断锁状态是否发生改变，从而判断之前copy到线程工作内存中的值是否与主内存的值存在不一致。</p>
<p>下图为StampedLock.validate方法的源码实现，通过锁标记与相关常量进行位运算、比较来校验锁状态，在校验逻辑之前，会通过Unsafe的loadFence方法加入一个load内存屏障，目的是避免上图用例中步骤②和StampedLock.validate中锁状态校验运算发生重排序导致锁状态校验不准确的问题。</p>
<figure data-type="image" tabindex="7"><img src="https://q456qq520.github.io/post-images/1682060654641.png" alt="" loading="lazy"></figure>
<h2 id="系统相关">系统相关</h2>
<p>这部分包含两个获取系统相关信息的方法。</p>
<pre><code class="language-java">//返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。
public native int addressSize();  
//内存页的大小，此值为2的幂次方。
public native int pageSize();
</code></pre>
<h3 id="典型应用-6">典型应用</h3>
<p>如下图所示的代码片段，为java.nio下的工具类Bits中计算待申请内存所需内存页数量的静态方法，其依赖于Unsafe中pageSize方法获取系统内存页大小实现后续计算逻辑。<br>
<img src="https://q456qq520.github.io/post-images/1682060743739.png" alt="" loading="lazy"></p>
<p>本文对Java中的sun.misc.Unsafe的用法及应用场景进行了基本介绍，我们可以看到Unsafe提供了很多便捷、有趣的API方法。即便如此，由于Unsafe中包含大量自主操作内存的方法，如若使用不当，会对程序带来许多不可控的灾难。因此对它的使用我们需要慎之又慎。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[BTrace]]></title>
        <id>https://q456qq520.github.io/post/btrace/</id>
        <link href="https://q456qq520.github.io/post/btrace/">
        </link>
        <updated>2023-04-21T05:32:18.000Z</updated>
        <summary type="html"><![CDATA[<p>A safe, dynamic tracing tool for the Java platform.<br>
适用于 Java 平台的安全动态跟踪工具。</p>
]]></summary>
        <content type="html"><![CDATA[<p>A safe, dynamic tracing tool for the Java platform.<br>
适用于 Java 平台的安全动态跟踪工具。</p>
<!-- more -->
<p>github地址：https://github.com/btraceio/btrace</p>
<h2 id="btrace是什么">BTrace是什么</h2>
<p>BTrace 可用于动态跟踪正在运行的 Java 程序（类似于 OpenSolaris 应用程序和操作系统的 DTrace）。BTrace 动态检测目标应用程序的类以注入跟踪代码（“字节码跟踪”）。</p>
<p>BTrace是基于Java语言的一个安全的、可提供动态追踪服务的工具。BTrace基于ASM、Java Attach Api、Instruments开发，为用户提供了很多注解。依靠这些注解，我们可以编写BTrace脚本（简单的Java代码）达到我们想要的效果，而不必深陷于ASM对字节码的操作中不可自拔。</p>
<p>看BTrace官方提供的一个简单例子：拦截所有java.io包中所有类中以read开头的方法，打印类名、方法名和参数名。当程序IO负载比较高的时候，就可以从输出的信息中看到是哪些类所引起，是不是很方便？</p>
<pre><code class="language-java">package com.sun.btrace.samples;

import com.sun.btrace.annotations.*;
import com.sun.btrace.AnyType;
import static com.sun.btrace.BTraceUtils.*;

/**
 * This sample demonstrates regular expression
 * probe matching and getting input arguments
 * as an array - so that any overload variant
 * can be traced in &quot;one place&quot;. This example
 * traces any &quot;readXX&quot; method on any class in
 * java.io package. Probed class, method and arg
 * array is printed in the action.
 */
@BTrace public class ArgArray {
    @OnMethod(
        clazz=&quot;/java\\.io\\..*/&quot;,
        method=&quot;/read.*/&quot;
    )
    public static void anyRead(@ProbeClassName String pcn, @ProbeMethodName String pmn, AnyType[] args) {
        println(pcn);
        println(pmn);
        printArray(args);
    }
}
</code></pre>
<p>再来看另一个例子：每隔2秒打印截止到当前创建过的线程数。</p>
<pre><code class="language-java">package com.sun.btrace.samples;

import com.sun.btrace.annotations.*;
import static com.sun.btrace.BTraceUtils.*;
import com.sun.btrace.annotations.Export;

/**
 * This sample creates a jvmstat counter and
 * increments it everytime Thread.start() is
 * called. This thread count may be accessed
 * from outside the process. The @Export annotated
 * fields are mapped to jvmstat counters. The counter
 * name is &quot;btrace.&quot; + &lt;className&gt; + &quot;.&quot; + &lt;fieldName&gt;
 */ 
@BTrace public class ThreadCounter {

    // create a jvmstat counter using @Export
    @Export private static long count;

    @OnMethod(
        clazz=&quot;java.lang.Thread&quot;,
        method=&quot;start&quot;
    ) 
    public static void onnewThread(@Self Thread t) {
        // updating counter is easy. Just assign to
        // the static field!
        count++;
    }

    @OnTimer(2000) 
    public static void ontimer() {
        // we can access counter as &quot;count&quot; as well
        // as from jvmstat counter directly.
        println(count);
        // or equivalently ...
        println(Counters.perfLong(&quot;btrace.com.sun.btrace.samples.ThreadCounter.count&quot;));
    }
}
</code></pre>
<p>除此之外，还可以做那些事情呢？</p>
<p>比如查看HashMap什么时候会触发rehash，以及此时容器中有多少元素等等。</p>
<h2 id="btrace架构">BTrace架构</h2>
<p>BTrace主要有下面几个模块：</p>
<ol>
<li>BTrace脚本：利用BTrace定义的注解，我们可以很方便地根据需要进行脚本的开发。</li>
<li>Compiler：将BTrace脚本编译成BTrace class文件。</li>
<li>Client：将class文件发送到Agent。</li>
<li>Agent：基于Java的Attach Api，Agent可以动态附着到一个运行的JVM上，然后开启一个BTrace Server，接收client发过来的BTrace脚本；解析脚本，然后根据脚本中的规则找到要修改的类；修改字节码后，调用Java Instrument的reTransform接口，完成对对象行为的修改并使之生效。</li>
</ol>
<p>整个BTrace的架构大致如下：<br>
<img src="https://q456qq520.github.io/post-images/1682056181598.jpeg" alt="" loading="lazy"></p>
<blockquote>
<p>名次解释：java.lang.instrument.Instrumentation<br>
redefineClasses和retransformClasses。一个是重新定义class，一个是修改class。都是替换已经存在的class文件，redefineClasses是自己提供字节码文件替换掉已存在的class文件retransformClasses是在已存在的字节码文件上修改后再替换之。当然，运行时直接替换类很不安全。比如新的class文件引用了一个不存在的类，或者把某个类的一个field给删除了等等，这些情况都会引发异常。所以如文档中所言，instrument存在诸多的限制。我们能做的基本上也就是简单修改方法内的一些行为。</p>
</blockquote>
<p>BTrace最终借Instruments实现class的替换。如上文所说，出于安全考虑，Instruments在使用上存在诸多的限制，BTrace也不例外。BTrace对JVM来说是“只读的”，因此BTrace脚本的限制如下：</p>
<ul>
<li>不允许创建对象</li>
<li>不允许创建数组</li>
<li>不允许抛异常</li>
<li>不允许catch异常</li>
<li>不允许随意调用其他对象或者类的方法，只允许调用com.sun.btrace.BTraceUtils中提供的静态方法（一些数据处理和信息输出工具）</li>
<li>不允许改变类的属性</li>
<li>不允许有成员变量和方法，只允许存在static public void方法</li>
<li>不允许有内部类、嵌套类</li>
<li>不允许有同步方法和同步块</li>
<li>不允许有循环</li>
<li>不允许随意继承其他类（当然，java.lang.Object除外）</li>
<li>不允许实现接口</li>
<li>不允许使用assert</li>
<li>不允许使用Class对象</li>
</ul>
<p>如此多的限制，其实可以理解。BTrace要做的是，虽然修改了字节码，但是除了输出需要的信息外，对整个程序的正常运行并没有影响。</p>
<h2 id="其他-arthas">其他-Arthas</h2>
<p>BTrace脚本在使用上有一定的学习成本，如果能把一些常用的功能封装起来，对外直接提供简单的命令即可操作的话，那就再好不过了。阿里的工程师们早已想到这一点，就在去年（2018年9月份），阿里巴巴开源了自己的Java诊断工具——<a href="https://github.com/alibaba/arthas">Arthas</a>。Arthas提供简单的命令行操作，功能强大。究其背后的技术原理，和本文中提到的大致无二。</p>
<p>Java的Instruments给运行时的动态追踪留下了希望，Attach API则给运行时动态追踪提供了“出入口”，ASM则大大方便了“人类”操作Java字节码的操作。</p>
<p>基于Instruments和Attach API前辈们创造出了诸如JProfiler、Jvisualvm、BTrace、Arthas这样的工具。以ASM为基础发展出了cglib、动态代理，继而是应用广泛的Spring AOP。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mac 安装Golang（一）]]></title>
        <id>https://q456qq520.github.io/post/mac-an-zhuang-golangyi/</id>
        <link href="https://q456qq520.github.io/post/mac-an-zhuang-golangyi/">
        </link>
        <updated>2023-04-04T07:34:55.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="一-下载">一 下载</h2>
<p>前往官网下载最新安装包，地址如下：<br>
https://golang.google.cn/dl/</p>
]]></summary>
        <content type="html"><![CDATA[<h2 id="一-下载">一 下载</h2>
<p>前往官网下载最新安装包，地址如下：<br>
https://golang.google.cn/dl/</p>
<!-- more -->
<p>下载完成，运行安装包。</p>
<h3 id="11-检查环境">1.1 检查环境</h3>
<ol>
<li>检查环境</li>
</ol>
<pre><code class="language-go"> go env
</code></pre>
<ol start="2">
<li>查看版本</li>
</ol>
<pre><code class="language-go">go version
</code></pre>
<ol start="3">
<li>查看目录<br>
<img src="https://q456qq520.github.io/post-images/1680594021632.png" alt="" loading="lazy"></li>
</ol>
<ul>
<li>bin: 存储可执行bin文件</li>
<li>pkg: 编译完成的文件</li>
<li>src: 源代码文件</li>
</ul>
<ol start="4">
<li>编辑环境变量<br>
如果需要修改环境变量，参考如下：</li>
</ol>
<pre><code class="language-go">vim ~/.bash_profile

# GOPATH配置为你的工作区目录
export GOROOT=/usr/local/go
export PATH=$PATH:$GOROOT/bin
export GOPATH=$HOME/go

source ~/.bash_profile
</code></pre>
<ol start="5">
<li>设置go国内模块代理</li>
</ol>
<pre><code class="language-go">vim ~/.bash_profile

export GO111MODULE=on
export GOPROXY=https://goproxy.cn
</code></pre>
<h2 id="二-运行">二 运行</h2>
<h3 id="21-创建一个go文件">2.1 创建一个go文件</h3>
<blockquote>
<p>提示: Go 语言源文件的拓展名以 .go 结尾。</p>
</blockquote>
<pre><code class="language-go">package main

import &quot;fmt&quot;

func main() {
    fmt.Println(&quot;Hello World !&quot;)
}
</code></pre>
<h3 id="22-执行-go-程序">2.2 执行 Go 程序</h3>
<h4 id="221-go-run">2.2.1 go run</h4>
<p>通过 go run 命令来执行刚刚的代码， 执行命令如下：</p>
<pre><code class="language-go">go run helloworld.go 
</code></pre>
<h5 id="222-go-bulid">2.2.2 go bulid</h5>
<p>还可以通过执行 go build 命令，将刚刚这段代码编译成可执行文件:</p>
<pre><code class="language-go">go build helloworld.go
</code></pre>
<p>编译完成后，可以在目录下看到一个 helloworld可执行文件，通过 ./helloworld 命令来执行它，即可输出 Hello World !</p>
<h2 id="三-jetbrains-golang-安装开发环境搭建">三 Jetbrains GoLang 安装&amp;开发环境搭建</h2>
<p>https://www.macyy.cn/archives/1157</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[面试题（三）]]></title>
        <id>https://q456qq520.github.io/post/mian-shi-ti-san/</id>
        <link href="https://q456qq520.github.io/post/mian-shi-ti-san/">
        </link>
        <updated>2023-03-12T04:28:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="mysql">mysql</h2>
<h3 id="1-什么是mysql中的降序索引">1. 什么是mysql中的降序索引</h3>
<p>降序索引是mysql8.0中才有的一种索引排序类型，默认为升序。</p>
<p>在MySQL中，降序索引是一种索引类型，可以帮助优化查询性能。与普通的升序索引不同，降序索引将索引键值按照降序排列，这意味着在查询时可以更快地找到符合条件的数据。</p>
<p>例如，假设有一个包含数百万条记录的表，其中有一个日期列，你经常需要按照日期倒序查找最新的数据。如果你在该列上创建了一个降序索引，则查询将非常快，因为MySQL将从索引的末尾开始查找最新的数据。</p>
<p>创建降序索引的语法与创建普通升序索引的语法类似，只需在索引定义中使用DESC关键字即可。例如，创建一个名为date_index的降序索引，可以使用以下语句：</p>
<pre><code class="language-mysql">CREATE INDEX date_index ON mytable (date_column DESC);
</code></pre>
<h2 id="线程池">线程池</h2>
<h3 id="1-线程池中线程是如何保活和回收的">1. 线程池中线程是如何保活和回收的</h3>
<p>线程保活：线程池中的线程通过调用线程管理器中的addWorker()方法来创建新的线程，这些线程被创建后会一直保活在线程池中。当任务到来时，线程池会通过线程管理器的getWorker()方法获取一个空闲线程来执行任务。线程池还会定期调用线程管理器的keepAlive()方法来检查空闲线程的数量，如果发现空闲线程过多，线程池会将多余的线程置为等待状态，以便在需要时能够立即响应任务。</p>
<p>线程回收：线程池中的线程在执行完任务后，并不会立即退出，而是会等待新的任务到来。线程池会通过线程管理器的getTask()方法获取任务，并将任务分配给空闲线程执行。如果线程空闲时间超过设定的时间，线程管理器会将该线程置为等待状态，并等待新的任务到来。如果等待的时间超过了设定的线程空闲时间，线程管理器会将该线程回收，并从线程池中移除该线程。</p>
<p>线程池的线程保活和回收机制是由线程管理器来管理的，线程管理器是线程池的核心组件之一。线程管理器通过对线程的创建、调度、回收等过程进行管理，保证线程池的运行效率和稳定性。同时，线程管理器还会通过使用锁、条件变量等机制来保证线程的安全性和可靠性。</p>
<h3 id="2-线程池有哪几种状态分别是如何变化的">2. 线程池有哪几种状态，分别是如何变化的</h3>
<p>Running（运行状态）：线程池处于正常运行状态，可以接受新的任务。<br>
Shutdown（关闭状态）：线程池不再接受新的任务，但会执行完已经提交的任务。<br>
Stop（停止状态）：线程池不再接受新的任务，并且会中断正在执行的任务。<br>
Tidying（整理状态）：所有的任务都已经执行完毕，工作线程数量为0，线程池将会转换到Terminated状态。<br>
Terminated（终止状态）：线程池彻底终止，不再处理任何任务。</p>
<p>线程池的状态转换通常是通过线程池的状态控制变量来实现的。例如，当线程池接收到shutdown()方法的调用时，线程池的状态会从Running状态转换为Shutdown状态。当线程池中的所有任务都执行完毕时，线程池的状态会从Tidying状态转换为Terminated状态。</p>
<h2 id="tomcat">tomcat</h2>
<h3 id="1-tomcat的最大线程数为什么默认是200">1. tomcat的最大线程数为什么默认是200</h3>
<p>Tomcat默认的最大线程数是200，这是一个经验值。具体来说，这个值取决于以下几个因素：</p>
<p>硬件资源：Tomcat所运行的服务器的硬件资源（如CPU和内存）越高，最大线程数就可以设置得更高。<br>
应用程序的负载：应用程序的负载越高，需要更多的线程来处理请求，最大线程数也就需要设置得更高。<br>
并发请求的处理时间：如果应用程序中的每个请求都需要大量的时间来处理，那么处理每个请求的线程就需要更长的时间。在这种情况下，最大线程数需要设置得更高，以便同时处理更多的请求。<br>
服务器的负载：如果Tomcat所运行的服务器还运行着其他的应用程序，那么这些应用程序也需要共享服务器的资源。因此，在这种情况下，最大线程数需要设置得更低，以免影响其他应用程序的运行。</p>
<p>总之，200作为Tomcat默认的最大线程数，是一个经验值，并不适用于所有情况。在实际应用中，最大线程数的设置应该根据应用程序的负载、硬件资源和服务器的负载等因素进行调整。</p>
<h2 id="分布式">分布式</h2>
<h3 id="1-什么是集群脑裂如何解决脑裂问题">1. 什么是集群脑裂，如何解决脑裂问题</h3>
<p>集群脑裂是指分布式系统中的节点之间失去联系或者通信故障，导致系统出现不一致的状态或无法提供服务的问题。举个例子，假设有一个由3个节点组成的集群，当节点1和节点2之间的通信故障时，这个集群就出现了脑裂问题，因为节点1和节点2之间的状态不一致会导致集群无法正常工作。</p>
<p>解决集群脑裂问题的方法有很多种，以下是一些常见的解决方法：</p>
<ol>
<li>心跳检测：在分布式系统中，通常使用心跳检测来检测节点之间的通信状态。当某个节点长时间无响应时，系统会将该节点视为已经失效，避免了因失效节点导致的脑裂问题。心跳检测可以使用UDP协议来实现，因为UDP协议的开销比TCP协议低，更适合在分布式系统中使用。</li>
<li>选举机制：在集群中选举一个“领导者”节点来负责处理请求，避免出现不一致的状态。当发生脑裂问题时，每个节点都会进行选举，选择一个新的领导者来处理请求。</li>
<li>数据复制：在分布式系统中，数据复制可以避免因为某个节点失效导致的脑裂问题。当某个节点失效时，可以使用其他节点上的备份数据来保证系统的一致性。</li>
<li>分布式锁：在分布式系统中，使用分布式锁可以避免因为不同节点之间的操作冲突导致的脑裂问题。例如，当多个节点同时对一个资源进行读写操作时，可以使用分布式锁来保证只有一个节点可以进行写操作，从而避免脑裂问题的发生。</li>
</ol>
<p>总之，解决集群脑裂问题的方法很多，具体选择哪种方法需要根据具体的应用场景和系统架构来进行评估和选择。</p>
<h3 id="2-微服物中什么是应用级注册什么是接口级注册优缺点是什么">2. 微服物中什么是应用级注册？什么是接口级注册？优缺点是什么</h3>
<p>在微服务架构中，应用级注册和接口级注册都是服务发现的方式，用于将服务注册到服务注册中心，以便其他服务或客户端可以发现和调用这些服务。</p>
<p>应用级注册是指将整个应用程序注册到服务注册中心，由服务注册中心负责维护该应用程序中所有的服务实例。在这种模式下，应用程序中的所有服务都共享同一个注册信息，服务注册中心可以自动维护服务实例的健康状态，并根据需要自动进行负载均衡和故障转移。应用级注册的优点是简单易用，适用于中小型的微服务应用场景，但缺点是不够灵活，如果一个应用程序中的某个服务实例出现故障，整个应用程序将会被标记为不可用，从而影响到其他服务的可用性。</p>
<p>接口级注册是指将每个服务实例注册到服务注册中心，并指定服务所提供的接口信息。在这种模式下，服务的注册信息更加细粒度，服务消费者可以根据具体的接口信息来发现和调用服务，而不必依赖于整个应用程序的注册信息。接口级注册的优点是更加灵活，服务实例之间相互独立，不会因为某个服务实例的故障而影响到整个应用程序的可用性，但缺点是注册信息的维护相对复杂，需要进行额外的配置和管理。</p>
<p>综上所述，应用级注册和接口级注册都有各自的优缺点，具体使用哪种注册方式需要根据实际的应用场景来进行选择和评估。对于大型的微服务应用场景，一般会采用接口级注册的方式来管理服务实例，以保证服务之间的独立性和灵活性。对于中小型的微服务应用场景，应用级注册的方式可以更加简单易用，快速搭建起微服务应用程序。</p>
<h2 id="框架">框架</h2>
<h3 id="1-springboot的自动配置是如何实现的">1. springboot的自动配置是如何实现的</h3>
<p>Spring Boot的自动配置机制的实现是基于Spring Framework的条件化配置机制，主要是通过使用@Conditional注解来实现的。</p>
<p>具体来说，自动配置是通过以下步骤实现的：</p>
<ol>
<li>Spring Boot在classpath中查找所有的spring.factories文件，并从这些文件中加载所有可用的自动配置类。</li>
<li>每个自动配置类都有一个或多个条件，这些条件使用@Conditional注解进行标注，以指示在何种情况下自动配置应该生效。</li>
<li>当Spring Boot应用程序启动时，Spring容器会根据条件化配置机制加载和实例化自动配置类中的bean。</li>
<li>自动配置类中的@Bean方法定义了应用程序所需的各种组件，这些组件可以是Spring容器管理的bean，例如数据源，事务管理器等。</li>
<li>根据应用程序的配置和条件，Spring Boot会选择性地启用或禁用自动配置类中的bean，这些条件可以是环境变量、系统属性、JVM参数等。</li>
<li>如果自动配置中的某些bean与应用程序中手动配置的bean冲突，那么手动配置的bean会覆盖自动配置的bean。</li>
</ol>
<p>Spring Boot的自动配置机制是基于Java Config的，它使用了许多Java Config的特性，例如使用@Bean注解定义bean，使用@Configuration注解标识配置类等。自动配置还利用了Spring的条件化配置机制来实现灵活的配置，这使得应用程序可以根据环境和需求自动选择需要的组件。</p>
<h3 id="2-如何设计一个rpc框架">2. 如何设计一个rpc框架</h3>
<p>设计一个RPC框架需要考虑以下几个方面：</p>
<ul>
<li>通信协议的设计。需要设计一个协议来定义消息的格式和内容，包括请求消息和响应消息。</li>
<li>服务注册与发现。需要设计一个机制来注册和发现可用的服务，以便客户端可以找到需要调用的服务。</li>
<li>序列化和反序列化。需要选择一种序列化方式来将对象序列化为字节流，并在接收方将字节流反序列化为对象。</li>
<li>网络通信。需要实现网络通信，包括客户端和服务端的通信。</li>
<li>负载均衡。需要设计一种负载均衡策略，以便在多个服务实例中选择一个最合适的实例进行调用。</li>
<li>异常处理。需要处理异常情况，例如网络连接失败、超时、服务不可用等情况。</li>
<li>安全认证。需要设计一种安全认证机制来保护数据的安全性。</li>
</ul>
<p>在实现RPC框架时，可以考虑使用以下技术：</p>
<ul>
<li>Java NIO或Netty等高性能的网络通信框架。</li>
<li>JSON或Protobuf等高效的序列化方式。</li>
<li>ZooKeeper或Consul等服务注册中心，以便实现服务注册和发现。</li>
<li>Ribbon或Nginx等负载均衡器。</li>
<li>Spring框架或Dubbo等RPC框架的实现方式，以便快速开发和管理。</li>
</ul>
<p>一个简单的RPC框架实现步骤如下：</p>
<ul>
<li>定义通信协议和消息格式。</li>
<li>定义服务接口和服务实现类，并使用注解将服务注册到注册中心。</li>
<li>实现客户端调用逻辑，包括负载均衡、序列化、网络通信等。</li>
<li>实现服务端接收请求逻辑，包括反序列化、调用服务实现类、序列化响应消息等。</li>
<li>添加异常处理、安全认证等扩展功能。</li>
<li>进行性能测试和调优，确保RPC框架的性能和稳定性。</li>
</ul>
<p>需要注意的是，RPC框架的设计和实现需要考虑很多细节，包括线程池管理、心跳机制、消息重试、请求超时等，因此需要进行充分的测试和验证，确保RPC框架的稳定性和可靠性。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringCloud微服务实战(二)]]></title>
        <id>https://q456qq520.github.io/post/springcloud-wei-fu-wu-shi-zhan-er/</id>
        <link href="https://q456qq520.github.io/post/springcloud-wei-fu-wu-shi-zhan-er/">
        </link>
        <updated>2023-03-07T08:22:24.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="四-服务容错保护-spring-cloud-hystrix">四 服务容错保护: Spring Cloud Hystrix</h2>
]]></summary>
        <content type="html"><![CDATA[<h2 id="四-服务容错保护-spring-cloud-hystrix">四 服务容错保护: Spring Cloud Hystrix</h2>
<!-- more -->
]]></content>
    </entry>
</feed>